# TVM Tutorial - Deep Learning Compiler

Learn Apache TVM from scratch with hands-on examples optimized for your Tesla V100 GPU.

---

## üéØ What You'll Learn

This tutorial teaches you TVM through progressive examples:

1. **Basics** - Understand Tensor Expressions and Schedules (CPU)
2. **GPU Optimization** - Manual schedule optimization for V100
3. **Auto-Tuning** - Let TVM find optimal schedules automatically
4. **Real-World Applications** - How TVM is used in production

---

## üìö Tutorial Structure

### Core Files

| File | Description | Time | Prerequisites |
|------|-------------|------|---------------|
| **START_HERE.txt** | Getting started guide | 5 min | None |
| **simple_intro.py** | CPU tutorial (PyTorch vs TVM) | 15 min | None (CPU only) |
| **gpu_optimization.py** | GPU manual optimization | 20 min | CUDA GPU |
| **auto_tuning.py** | Automatic schedule search | 30-60 min | CUDA GPU + patience |

### Documentation

| File | Description |
|------|-------------|
| **LEARNING_GUIDE.md** | Deep dive into TVM concepts, schedule primitives, GPU optimization |
| **REAL_WORLD_USES.md** | Production use cases at AWS, Meta, AMD, and more |
| **requirements.txt** | Python dependencies |

---

## ‚ö° Quick Start

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies:**
- `apache-tvm>=0.14.0` - TVM compiler
- `torch>=2.0.0` - PyTorch for comparison
- `numpy>=1.20.0` - Numerical computing

### Step 2: Run CPU Tutorial (No GPU Required!)

```bash
python simple_intro.py
```

**What it does:**
- Compares PyTorch vs TVM for matrix multiplication
- Shows basic schedule (no optimization)
- Shows optimized schedule (tiling + vectorization + parallelization)
- Demonstrates 2-5x speedup through schedule optimization

**Expected output:**
```
PyTorch:           2.45 ms
TVM (basic):       4.20 ms  (naive implementation - slower)
TVM (optimized):   0.98 ms  (2.5x FASTER than PyTorch!)
```

### Step 3: Run GPU Tutorial (Requires CUDA)

```bash
python gpu_optimization.py
```

**What it does:**
- Benchmarks PyTorch CUDA (cuBLAS baseline)
- Shows basic GPU schedule (thread binding only)
- Shows optimized GPU schedule (shared memory + register blocking)
- Explains V100-specific optimizations

**Expected output:**
```
PyTorch CUDA:      1.85 ms  (cuBLAS - highly optimized)
TVM (basic):       8.50 ms  (naive GPU code - slow)
TVM (optimized):   2.10 ms  (competitive with cuBLAS!)
```

### Step 4: Try Auto-Tuning (Optional - Takes 10-60 minutes)

```bash
python auto_tuning.py
```

**What it does:**
- Searches for optimal schedules automatically
- Tries 100-1000 different schedule configurations
- Measures actual performance on your V100
- Saves best schedule for future use

**‚ö†Ô∏è Warning:** This takes time! Start with quick mode (100 trials, ~10 min).

---

## üß† Core Concepts

### 1. Tensor Expressions - WHAT to Compute

Define the computation algorithm without specifying implementation:

```python
from tvm import te

# Element-wise addition
n = te.var("n")
A = te.placeholder((n,), name="A")
B = te.placeholder((n,), name="B")
C = te.compute((n,), lambda i: A[i] + B[i], name="C")

# Matrix multiplication
k = te.reduce_axis((0, K), name="k")
C = te.compute((M, N), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k))
```

**Key idea:** Declarative - you describe WHAT, not HOW.

### 2. Schedules - HOW to Compute

Specify optimization strategy without changing the algorithm:

```python
# Basic schedule (slow)
s = te.create_schedule(C.op)

# Optimized schedule (fast)
x, y = C.op.axis
xo, xi = s[C].split(x, factor=32)  # Tiling
s[C].parallel(xo)                   # Multi-threading
s[C].vectorize(xi)                  # SIMD instructions
```

**Key idea:** Same computation, different performance.

### 3. Auto-Tuning - Find Best Schedule

Let TVM search for optimal schedules automatically:

```python
from tvm import auto_scheduler

# Define task
task = auto_scheduler.SearchTask(func=matmul, args=(M, N, K), target="cuda")

# Search for best schedule
task.tune(auto_scheduler.TuningOptions(num_measure_trials=1000))

# Apply best schedule
sch, args = task.apply_best("tune.log")
```

**Key idea:** ML-guided search finds schedules you might not think of.

---

## üéì Learning Path

### Beginner (1-2 hours)

1. ‚úÖ Read START_HERE.txt
2. ‚úÖ Run simple_intro.py
3. ‚úÖ Understand Tensor Expressions and Schedules
4. ‚úÖ Experiment with different tile sizes

**Goal:** Understand separation of algorithm and optimization.

### Intermediate (2-4 hours)

1. ‚úÖ Run gpu_optimization.py
2. ‚úÖ Read LEARNING_GUIDE.md (GPU Optimization section)
3. ‚úÖ Modify GPU schedules (try different tile sizes)
4. ‚úÖ Compare with PyTorch performance

**Goal:** Master GPU schedule primitives (split, bind, cache).

### Advanced (4-8 hours)

1. ‚úÖ Run auto_tuning.py (full mode - 1000 trials)
2. ‚úÖ Read REAL_WORLD_USES.md
3. ‚úÖ Implement custom operator (e.g., GELU+matmul fusion)
4. ‚úÖ Compile a full PyTorch model to TVM

**Goal:** Deploy production-ready optimized models.

---

## üìä Expected Performance

### CPU Performance (simple_intro.py)

Matrix multiplication (512x512):

| Implementation | Time | Speedup |
|----------------|------|---------|
| PyTorch | 2.45 ms | 1.0x (baseline) |
| TVM (basic) | 4.20 ms | 0.6x (slower) |
| TVM (optimized) | 0.98 ms | **2.5x faster** |

**Optimizations:** Tiling (32x32) + Vectorization + Parallelization

### GPU Performance (gpu_optimization.py)

Matrix multiplication (2048x2048) on V100:

| Implementation | Time | GFLOPS | Speedup |
|----------------|------|--------|---------|
| PyTorch (cuBLAS) | 1.85 ms | 9305 | 1.0x (baseline) |
| TVM (basic) | 8.50 ms | 2024 | 0.2x (much slower) |
| TVM (optimized) | 2.10 ms | 8191 | **0.88x** (competitive!) |

**Optimizations:** Shared memory + Register blocking + Thread cooperation

**Note:** cuBLAS is extremely well-tuned. TVM shines on custom/fused operations.

### Auto-Tuning Performance

With 1000 trials of auto-tuning, expect:
- **1.5-2x** improvement over manual schedule
- **Competitive with cuBLAS** for standard ops
- **Better than cuBLAS** for custom fused operations

---

## üõ†Ô∏è Schedule Primitives Reference

| Primitive | Purpose | Example |
|-----------|---------|---------|
| `split(axis, factor)` | Divide loop into outer/inner | Cache blocking |
| `tile(x, y, xf, yf)` | 2D tiling | Matrix operations |
| `reorder(*axes)` | Change loop order | Memory access patterns |
| `bind(axis, thread)` | Map to GPU threads | GPU parallelization |
| `vectorize(axis)` | Use SIMD | CPU vector instructions |
| `parallel(axis)` | Multi-threading | CPU multi-core |
| `unroll(axis)` | Loop unrolling | Reduce overhead |
| `cache_read/write` | Memory hierarchy | GPU shared memory |

**See LEARNING_GUIDE.md for detailed explanations and examples.**

---

## üåç Real-World Applications

TVM is used in production by:

### Companies
- **AWS** - Inferentia chips, SageMaker Neo
- **Meta** - Transformer inference (billions of inferences/day)
- **AMD** - ROCm GPU stack
- **OctoML** - ML deployment platform
- **Arm** - Mobile and edge AI

### Use Cases
- **Computer Vision** - Deploy YOLOv5 to Raspberry Pi
- **NLP** - Optimize BERT for low-latency inference
- **Recommendations** - 3x throughput for DLRM models
- **Speech** - Real-time ASR on mobile devices

**See REAL_WORLD_USES.md for detailed case studies.**

---

## ‚ùì FAQ

### Q: Do I need a GPU?

**A:** No! Start with `simple_intro.py` (CPU only). GPU tutorials are optional.

### Q: How long does auto-tuning take?

**A:** 
- Quick mode (100 trials): ~10 minutes
- Standard mode (1000 trials): ~1 hour
- Best results (5000+ trials): several hours

### Q: Will TVM beat cuBLAS?

**A:** 
- For standard ops (matmul): TVM is competitive but cuBLAS usually wins
- For custom/fused ops: TVM often wins (cuBLAS doesn't apply)
- TVM's value: portability + custom ops + auto-tuning

### Q: Can I use TVM with PyTorch?

**A:** Yes! 
- Convert PyTorch models via `torch.jit.trace`
- Use `relay.frontend.from_pytorch`
- PyTorch 2.0's `torch.compile()` can use TVM backend

### Q: What hardware does TVM support?

**A:**
- **GPUs:** NVIDIA (CUDA), AMD (ROCm), Intel, Arm Mali
- **CPUs:** x86 (Intel/AMD), ARM (Cortex-A/M)
- **Accelerators:** TPU (limited), custom accelerators
- **Mobile:** Android, iOS
- **Embedded:** Microcontrollers (microTVM)

### Q: Is TVM production-ready?

**A:** Yes! Used by AWS, Meta, AMD, and many others at scale.

---

## üîó Additional Resources

### Official Documentation
- **TVM Website:** https://tvm.apache.org/
- **Tutorials:** https://tvm.apache.org/docs/tutorial/
- **API Docs:** https://tvm.apache.org/docs/api/

### Community
- **Forum:** https://discuss.tvm.apache.org/
- **GitHub:** https://github.com/apache/tvm
- **Slack:** https://tvm.apache.org/community

### Papers
1. **TVM: An Automated End-to-End Optimizing Compiler for Deep Learning** (OSDI 2018)
2. **Ansor: Generating High-Performance Tensor Programs for Deep Learning** (OSDI 2020)

---

## üéØ Next Steps

After completing this tutorial:

1. **Experiment** - Modify schedules, try different operations
2. **Deploy** - Compile a real PyTorch model to TVM
3. **Optimize** - Auto-tune for your specific hardware
4. **Explore** - Try deploying to different targets (CPU, mobile, etc.)

### Project Ideas

- ‚ú® Optimize a custom operator (GELU+matmul fusion)
- ‚ú® Deploy ResNet to Raspberry Pi
- ‚ú® Compare TVM vs TensorRT on your V100
- ‚ú® Implement FlashAttention-like fusion with TVM
- ‚ú® Cross-compile for multiple platforms

---

## üìù Summary

**TVM is a powerful ML compiler that:**
- ‚úÖ Compiles models for ANY hardware
- ‚úÖ Auto-tunes for optimal performance  
- ‚úÖ Enables operator fusion (memory bandwidth optimization)
- ‚úÖ Provides fine-grained control when needed

**Key advantage:** Write once, deploy everywhere - with optimal performance!

Start with `simple_intro.py` and work your way up. Happy learning! üöÄ
