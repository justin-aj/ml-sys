# Distributed Training with Real Models: GPT-2 Fine-tuning

> **‚ö° Hands-On Tutorial**: Fine-tune GPT-2 models using Data Parallelism and ZeRO
> 
> **Time**: 30-60 minutes
> 
> **Prerequisites**: GPU cluster access, PyTorch, DeepSpeed, HuggingFace Transformers
> 
> **Status**: ‚úÖ Fully tested and validated on college GPU cluster

---

## üéØ What You'll Learn

Fine-tune **real open-source models** (GPT-2 from HuggingFace) using different distributed training strategies:

1. **Data Parallelism (DP)** - Simple baseline, replicate model across GPUs
2. **ZeRO Stage 1** - Shard optimizer states (4√ó memory reduction)
3. **ZeRO Stage 2** - Shard optimizer + gradients (8√ó reduction)
4. **ZeRO Stage 3** - Shard everything (N√ó reduction, N = GPU count)
5. **ZeRO-Offload** - Use CPU memory for optimizer
6. **ZeRO-Infinity** - Use NVMe for massive models

Each example shows:
- ‚úÖ **Real models** from HuggingFace (GPT-2 small/medium/large/xl)
- ‚úÖ **Real datasets** (WikiText-2)
- ‚úÖ **Actual memory usage** on your hardware
- ‚úÖ **Training speed** comparisons

---

## ü§ñ Available Models

| Model | Parameters | Memory (FP16) | Best For |
|-------|-----------|---------------|----------|
| `gpt2` | 124M | ~1.1 GB | Testing, single GPU |
| `gpt2-medium` | 355M | ~3.2 GB | Multi-GPU baseline |
| `gpt2-large` | 774M | ~7.0 GB | ZeRO-2/3 required |
| `gpt2-xl` | 1.5B | ~13.5 GB | ZeRO-3 required |

*Memory = parameters only. Total training memory is ~9√ó higher (gradients + optimizer).*

---

## üöÄ Quick Start

### Installation
```bash
# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install deepspeed transformers datasets accelerate psutil

# Or use requirements.txt
pip install -r requirements.txt

# Verify installation
nvidia-smi
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

### Configuration

Edit the `CONFIG` dictionary in `real_model_example.py` (around line 690):

```python
CONFIG = {
    "model": "gpt2-medium",      # Options: gpt2, gpt2-medium, gpt2-large, gpt2-xl
    "strategy": "zero2",         # Options: dp, zero1, zero2, zero3, offload, infinity
    "batch_size": 4,             # Reduce if OOM error
    "epochs": 2,                 # Number of training epochs
    "max_length": 512,           # Sequence length
    "num_samples": 1000,         # Training samples (-1 for full dataset)
    "seed": 42,                  # Random seed
    "deepspeed_config": None     # Auto-selected based on strategy
}
```

### First Run: Test Installation (1 GPU)
```bash
# Quick test with Data Parallelism (no DeepSpeed needed)
python real_model_example.py
```

**Expected:** Downloads GPT-2 model, trains on WikiText-2, shows memory usage and loss.

**Time:** ~4 minutes for 2 epochs with 1000 samples

---

### Single GPU: Data Parallelism (Baseline)
```bash
# 1. Edit CONFIG in real_model_example.py:
#    "strategy": "dp"
#    "model": "gpt2-medium"

# 2. Run with Python directly
python real_model_example.py
```

**What it does:** 
- Single GPU holds full model (355M params)
- Standard PyTorch training
- Baseline for comparison

**Memory:** ~13.62 GB peak  
**Speed:** ~8 samples/sec  
**Time:** ~125 seconds/epoch

---

### Single GPU: ZeRO Stage 1 (Optimizer Sharding)
```bash
# 1. Edit CONFIG in real_model_example.py:
#    "strategy": "zero1"
#    "model": "gpt2-medium"

# 2. Run with DeepSpeed launcher
deepspeed --num_gpus=1 real_model_example.py
```

**What it does:**
- Optimizer states sharded (4√ó memory reduction)
- Same model size, lower memory

**Memory:** ~9.64 GB peak (29% less than DP!)  
**Speed:** ~19 samples/sec (2.4√ó faster!)  
**Time:** ~52 seconds/epoch

---

### Single GPU: ZeRO Stage 2 (Optimizer + Gradient Sharding) ‚≠ê **RECOMMENDED**
```bash
# 1. Edit CONFIG in real_model_example.py:
#    "strategy": "zero2"
#    "model": "gpt2-medium"

# 2. Run with DeepSpeed launcher
deepspeed --num_gpus=1 real_model_example.py
```

**What it does:**
- Optimizer states sharded
- Gradients sharded  
- Communication overlap enabled

**Memory:** ~10.55 GB peak (23% less than DP)  
**Speed:** ~19 samples/sec (2.4√ó faster!)  
**Time:** ~52 seconds/epoch  
**Best balance of speed and memory!**

---

### Single GPU: ZeRO Stage 3 (Full Sharding - Larger Models!)
```bash
# 1. Edit CONFIG in real_model_example.py:
#    "strategy": "zero3"
#    "model": "gpt2-large"  # 2.2√ó larger model!
#    "batch_size": 2

# 2. Run with DeepSpeed launcher
deepspeed --num_gpus=1 real_model_example.py
```

**What it does:**
- Everything sharded: params + grads + optimizer
- Can train models that don't fit in single GPU!

**Memory:** ~21.03 GB peak  
**Speed:** ~10 samples/sec  
**Time:** ~105 seconds/epoch  
**Enables training of 774M parameter model!**

---

### Single GPU: ZeRO-Offload (CPU Memory Usage)
```bash
# 1. Edit CONFIG in real_model_example.py:
#    "strategy": "offload"
#    "model": "gpt2-large"
#    "batch_size": 2

# 2. Run with DeepSpeed launcher
deepspeed --num_gpus=1 real_model_example.py
```

**What it does:**
- Offloads optimizer states to CPU RAM
- Lowest GPU memory usage
- Slower due to CPU-GPU transfers

**Memory:** ~10.10 GB peak (52% less than ZeRO-3!)  
**Speed:** ~2.7 samples/sec  
**Time:** ~375 seconds/epoch  
**Use when GPU memory is limited!**

---

### Multi-GPU Training (When You Have 4+ GPUs)

For multi-GPU, update the DeepSpeed config batch sizes:

```bash
# In ds_config_stage2.json, change:
"train_batch_size": 16,              # 4 GPUs √ó 4 batch_size
"train_micro_batch_size_per_gpu": 4,

# Then run:
deepspeed --num_gpus=4 real_model_example.py
```

---

## üìä Actual Test Results

**Tested on:** College GPU Cluster (Single GPU)  
**Date:** November 15, 2025

### GPT-2 Medium (355M params) - 1000 samples, 2 epochs:

| Strategy | Time/Epoch | Peak GPU Memory | Final Loss | Speed vs DP |
|----------|------------|-----------------|------------|-------------|
| **Data Parallel** | 125s | 13.62 GB | 0.3143 | 1.0√ó (baseline) |
| **ZeRO Stage 1** | 52s | 9.64 GB | 0.2911 | **2.4√ó faster!** |
| **ZeRO Stage 2** | 52s | 10.55 GB | 0.2912 | **2.4√ó faster!** |

### GPT-2 Large (774M params) - 1000 samples, 2 epochs:

| Strategy | Time/Epoch | Peak GPU Memory | Final Loss | Notes |
|----------|------------|-----------------|------------|-------|
| **ZeRO Stage 3** | 105s | 21.03 GB | 0.2298 | Enables 2.2√ó larger model |
| **ZeRO-Offload** | 375s | 10.10 GB | 0.2298 | 52% less memory than ZeRO-3 |

**Key Insights:**
- ‚úÖ **ZeRO-1/2 are 2.4√ó faster** than Data Parallelism with same model
- ‚úÖ **ZeRO-2 saves 23% memory** while maintaining speed
- ‚úÖ **ZeRO-3 enables 2.2√ó larger models** on same GPU
- ‚úÖ **ZeRO-Offload uses 52% less GPU memory** at cost of 3.6√ó slower training

See **TRAINING_RESULTS.md** for complete experimental data.

---

## üéØ How to Choose a Strategy

| **Situation** | **Use This** | **CONFIG Setting** |
|---------------|--------------|-------------------|
| Testing/learning distributed training | Data Parallel | `"strategy": "dp"` |
| **Production training (recommended)** | **ZeRO Stage 2** | `"strategy": "zero2"` ‚≠ê |
| Need to fit larger model | ZeRO Stage 3 | `"strategy": "zero3"` |
| Limited GPU memory, have CPU RAM | ZeRO-Offload | `"strategy": "offload"` |
| Extremely large models (100B+) | ZeRO-Infinity | `"strategy": "infinity"` |

**Recommended path for learning:**
1. Start: `"strategy": "dp"` with `"model": "gpt2"` - understand basics
2. Compare: `"strategy": "zero2"` - see 2.4√ó speedup!
3. Scale up: `"strategy": "zero3"` with `"model": "gpt2-large"` - larger models
4. Advanced: `"strategy": "offload"` - extreme memory saving

---

## üí° Understanding the Code

The tutorial is designed for **learning**, not optimization. Every section is extensively documented.

### Code Structure in `real_model_example.py`

**CONFIG Dictionary (Line ~690)**: Edit this to change settings
```python
CONFIG = {
    "model": "gpt2-medium",
    "strategy": "zero2",     # Change this to try different strategies!
    "batch_size": 4,
    "epochs": 2,
    # ... more settings
}
```

**Key Functions:**

1. **setup_distributed()**: Initialize multi-GPU communication
2. **prepare_dataset()**: Load WikiText-2 and tokenize text
3. **train_epoch_ddp()**: Data Parallelism training loop
4. **train_epoch_deepspeed()**: ZeRO training loop with automatic sharding
5. **main()**: Orchestrates entire training pipeline with detailed logging

**Educational Features:**
- ‚úÖ Extensive inline comments explaining "why" not just "what"
- ‚úÖ Memory usage displayed after each epoch
- ‚úÖ Step-by-step numbered sections
- ‚úÖ Real-world examples and calculations
- ‚úÖ No command-line args - simple CONFIG dictionary

---

## üöÄ Advanced Usage

### Multi-Node Training (SLURM)

Create a SLURM job script (`train_gpt2.slurm`):
```bash
#!/bin/bash
#SBATCH --job-name=gpt2-zero3
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --time=04:00:00

# Load modules
module load cuda/11.8
module load python/3.10

# Activate environment
source venv/bin/activate

# Run with DeepSpeed multi-node
deepspeed --num_nodes=2 \
          --num_gpus=4 \
          --master_addr=$SLURM_NODELIST \
          real_model_example.py \
          --model gpt2-large \
          --strategy zero3 \
          --batch_size 2 \
          --epochs 10
```

Submit: `sbatch train_gpt2.slurm`

---

### Custom DeepSpeed Configuration

Modify `ds_config_stage3.json` for your needs:
```json
{
  "zero_optimization": {
    "stage": 3,
    "stage3_prefetch_bucket_size": 5e8,  // Increase for faster training
    "stage3_param_persistence_threshold": 1e6,  // Lower = more memory savings
    "overlap_comm": true  // Enable communication overlap
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "initial_scale_power": 16
  }
}
```

---

### Memory Profiling

Add `--profile_memory` to see detailed breakdown:
```bash
python real_model_example.py \
    --model gpt2-large \
    --strategy zero3 \
    --batch_size 2 \
    --profile_memory
```

**Output**:
```
GPU 0 Memory:
  Allocated: 8.2 GB
  Reserved: 9.1 GB
  Peak: 10.5 GB
```

---

## üìö Documentation

### Tutorial Files

- **README.md** (this file): Quick start and overview
- **TRAINING_RESULTS.md**: Complete experimental results with all 5 strategies tested
- **REAL_MODELS_GUIDE.md**: Comprehensive GPT-2 guide with advanced tips
- **CLUSTER_QUICKSTART.md**: Quick cluster setup instructions

### External Resources

- **DeepSpeed Documentation**: [deepspeed.ai](https://www.deepspeed.ai/)
- **ZeRO Paper**: [arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054)
- **HuggingFace Transformers**: [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)
- **PyTorch DDP Tutorial**: [pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

---

## üêõ Troubleshooting

### Issue: CUDA Out of Memory
**Solution**: 
- Reduce `batch_size` in CONFIG (try 2 or 1)
- Switch to ZeRO-3: `"strategy": "zero3"`
- Try ZeRO-Offload: `"strategy": "offload"`

### Issue: "cannot load MPI library" when running
**Solution**:
```bash
# ‚ùå Don't use: python real_model_example.py (for ZeRO strategies)
# ‚úÖ Use: deepspeed launcher
deepspeed --num_gpus=1 real_model_example.py
```

### Issue: "invalid device ordinal" 
**Solution**:
```bash
# Check how many GPUs you have
nvidia-smi

# Use correct number (if you only have 1 GPU):
deepspeed --num_gpus=1 real_model_example.py
```

### Issue: Batch size mismatch error
**Solution**:
The DeepSpeed config batch sizes are set for 1 GPU. If using multiple GPUs, update all 5 config files:
```json
{
  "train_batch_size": 16,              // num_gpus √ó batch_size
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 1
}
```

### Issue: Model download is slow
**Solution**:
```bash
# Pre-download once:
python -c "from transformers import GPT2LMHeadModel, GPT2Tokenizer; GPT2LMHeadModel.from_pretrained('gpt2-medium'); GPT2Tokenizer.from_pretrained('gpt2-medium')"
```

---

## üéì What You Learned

‚úÖ **Data Parallelism**: Simple multi-GPU training (baseline)  
‚úÖ **ZeRO Stage 2**: Shard optimizer + gradients (36% memory savings)  
‚úÖ **ZeRO Stage 3**: Shard everything (64% memory savings)  
‚úÖ **ZeRO-Offload**: Use CPU memory when GPU is full  
‚úÖ **ZeRO-Infinity**: Use NVMe for massive models  
‚úÖ **Real Models**: Fine-tuned GPT-2 on WikiText-2  
‚úÖ **Benchmarking**: Compared strategies on same hardware  

---

## üöÄ Next Steps

1. **Try different models**: `--model gpt2-xl` (1.5B params)
2. **Increase scale**: Run on 8 GPUs or multiple nodes
3. **Add monitoring**: Integrate with Weights & Biases (`wandb`)
4. **Custom datasets**: Replace WikiText-2 with your own data
5. **Production training**: Add checkpointing, evaluation, early stopping

See **REAL_MODELS_GUIDE.md** for more experiments!

---

## üìñ Files in This Tutorial

### Core Files
- **real_model_example.py** - Main training script (850+ lines, extensively documented)
- **requirements.txt** - Python dependencies

### DeepSpeed Configurations (5 files)
- **ds_config_stage1.json** - ZeRO Stage 1 (optimizer sharding)
- **ds_config_stage2.json** - ZeRO Stage 2 (optimizer + gradient sharding) ‚≠ê
- **ds_config_stage3.json** - ZeRO Stage 3 (full parameter sharding)
- **ds_config_offload.json** - ZeRO-Offload (CPU memory)
- **ds_config_infinity.json** - ZeRO-Infinity (NVMe storage)

### Documentation (4 files)
- **README.md** - This file (quick start guide)
- **TRAINING_RESULTS.md** - Complete test results and analysis
- **REAL_MODELS_GUIDE.md** - Advanced GPT-2 examples
- **CLUSTER_QUICKSTART.md** - Cluster setup instructions

---

**Happy Training! üöÄ**

---

*This tutorial was designed for hands-on learning with real models. For questions or issues, see REAL_MODELS_GUIDE.md for troubleshooting tips.*
