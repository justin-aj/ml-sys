# 3D Parallelism: Combining All Dimensions

## Introduction

**3D Parallelism** is the combination of three orthogonal parallelization strategies:
1. **Data Parallelism** (D) - Across data batches
2. **Pipeline Parallelism** (P) - Across model layers
3. **Tensor Parallelism** (T) - Within individual layers

This document explains how they work together to enable training of trillion-parameter models.

---

## The Three Dimensions

### Dimension 1: Data Parallelism (D)

**What it does**: Replicates the entire model and processes different data batches

```
Original batch size: 512
Data parallel degree: D = 4
Micro-batch per GPU: 512 / 4 = 128

GPU Group 0: Processes samples 0-127
GPU Group 1: Processes samples 128-255
GPU Group 2: Processes samples 256-383
GPU Group 3: Processes samples 384-511

After forward/backward:
â””â”€ ALL-REDUCE gradients across all groups
â””â”€ All groups update with same gradients
```

**When to use**:
- Model fits in GPU memory (with pipeline/tensor parallel)
- Want to increase effective batch size
- Have multiple copies of the model

### Dimension 2: Pipeline Parallelism (P)

**What it does**: Splits model layers into sequential stages

```
96-layer model, P = 4 stages:

Stage 0 (GPU 0-N):  Layers 0-23
Stage 1 (GPU N-2N): Layers 24-47
Stage 2 (GPU 2N-3N): Layers 48-71
Stage 3 (GPU 3N-4N): Layers 72-95

Data flows: Stage 0 â†’ Stage 1 â†’ Stage 2 â†’ Stage 3
```

**When to use**:
- Model depth doesn't fit in memory
- Have sequential layer structure
- Can tolerate pipeline bubbles

### Dimension 3: Tensor Parallelism (T)

**What it does**: Splits individual layers across GPUs

```
Single attention layer, T = 8:

GPU 0: Attention heads 0-11
GPU 1: Attention heads 12-23
GPU 2: Attention heads 24-35
...
GPU 7: Attention heads 84-95

All GPUs compute in parallel for one layer
```

**When to use**:
- Individual layers don't fit in memory
- Model width is very large
- Have high-bandwidth interconnect (NVLink)

---

## How They Combine: The 3D Cube

### Visualization

```
                Pipeline Dimension (P=4)
                      â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           /â”‚  S0    S1    S2    S3â”‚
          / â”‚ L0-23 L24-47 L48-71 L72-95
         /  â”‚                      â”‚
        /   â”‚      Each stage has  â”‚
       /    â”‚      T=8 GPUs for   â”‚
      /     â”‚      tensor parallel â”‚
     /      â”‚                      â”‚
    /       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   /       /                      /
  /       /  Tensor Parallel     /
 /       /   Dimension (T=8)    /
/       /         â†“            /
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  This entire cube is         â”‚
â”‚  replicated D=2 times for    â”‚
â”‚  data parallelism            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
    Data Parallel Dimension (D=2)

Total GPUs = D Ã— P Ã— T = 2 Ã— 4 Ã— 8 = 64
```

### GPU Assignment

```
For D=2, P=4, T=8 (64 total GPUs):

Data Replica 0:
â”œâ”€ Pipeline Stage 0: GPUs 0-7   (tensor parallel)
â”œâ”€ Pipeline Stage 1: GPUs 8-15  (tensor parallel)
â”œâ”€ Pipeline Stage 2: GPUs 16-23 (tensor parallel)
â””â”€ Pipeline Stage 3: GPUs 24-31 (tensor parallel)

Data Replica 1:
â”œâ”€ Pipeline Stage 0: GPUs 32-39 (tensor parallel)
â”œâ”€ Pipeline Stage 1: GPUs 40-47 (tensor parallel)
â”œâ”€ Pipeline Stage 2: GPUs 48-55 (tensor parallel)
â””â”€ Pipeline Stage 3: GPUs 56-63 (tensor parallel)
```

---

## Memory Distribution

### Example: GPT-3 175B on 1024 GPUs

Configuration: D=8, P=16, T=8

```
Total Parameters: 175 billion
Parameter Memory: 175B Ã— 4 bytes (FP32) = 700 GB

Distribution:
â”œâ”€ Pipeline splits 96 layers into 16 stages
â”‚  â””â”€ Each stage: 6 layers
â”‚
â”œâ”€ Tensor parallel splits each layer across 8 GPUs
â”‚  â””â”€ Parameters per GPU: 700 GB / (16 Ã— 8) = ~5.5 GB
â”‚
â””â”€ Data parallel replicates 8 times
   â””â”€ No additional memory (same parameters)

Memory per GPU breakdown:
â”œâ”€ Model parameters: ~5.5 GB
â”œâ”€ Gradients: ~5.5 GB
â”œâ”€ Optimizer states (Adam): ~16.5 GB (3Ã— parameters)
â”œâ”€ Activations: ~10 GB (depends on micro-batch size)
â”œâ”€ Working memory: ~2.5 GB
â””â”€ Total: ~40 GB per GPU âœ… Fits on A100 (80GB)
```

### Memory Scaling Law

```
For model with M parameters on DÃ—PÃ—T GPUs:

Memory per GPU â‰ˆ (M Ã— 4 bytes) / (P Ã— T)
                 + activations
                 + optimizer overhead

Pipeline (P): Divides layers
Tensor (T): Divides layer width
Data (D): Doesn't affect memory (replicates model)
```

---

## Communication Patterns

### Three Independent Communication Groups

```
1. Tensor Parallel Group (T=8):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ GPU 0 â†” GPU 1 â†” ... â†” GPU 7â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Communication: ALL-REDUCE
   Frequency: 2Ã— per layer
   Volume: O(batch_size Ã— seq_len Ã— hidden)
   Requirement: NVLink (high bandwidth, low latency)

2. Pipeline Parallel Group (P=4):
   GPU Set 0 â†’ GPU Set 1 â†’ GPU Set 2 â†’ GPU Set 3
   Communication: POINT-TO-POINT (send/recv)
   Frequency: Per microbatch
   Volume: O(batch_size Ã— seq_len Ã— hidden)
   Requirement: InfiniBand (good bandwidth)

3. Data Parallel Group (D=2):
   Replica 0 GPUs â†” Replica 1 GPUs
   Communication: ALL-REDUCE (gradients)
   Frequency: Once per training step
   Volume: O(model_parameters / (P Ã— T))
   Requirement: InfiniBand (can overlap with computation)
```

### Communication Hierarchy

```
Most Frequent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Least Frequent
Highest BW Req â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Lowest BW Req

Tensor Parallel    Pipeline Parallel    Data Parallel
     (T)                 (P)                 (D)
      â†“                   â†“                   â†“
   NVLink            InfiniBand          InfiniBand
   2Ã—/layer          per microbatch      per step
   ~5-10% overhead   ~10-15% overhead    ~3-5% overhead
```

---

## Training Step Execution

### Forward Pass

```
For microbatch m:

1. Data Parallel: Each replica processes different data
   Replica 0: microbatch mâ‚€
   Replica 1: microbatch mâ‚
   ...

2. Pipeline Parallel: Flow through stages
   Stage 0 processes mâ‚€ â†’ sends to Stage 1
   Stage 1 processes mâ‚€ â†’ sends to Stage 2
   Stage 2 processes mâ‚€ â†’ sends to Stage 3
   Stage 3 processes mâ‚€ â†’ computes loss

3. Tensor Parallel: Within each stage
   For each layer in stage:
   â”œâ”€ Split computation across T GPUs
   â”œâ”€ ALL-REDUCE after attention
   â””â”€ ALL-REDUCE after FFN

Timeline (Pipeline stages working on different microbatches):
Time â†’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0: [mâ‚€] [mâ‚] [mâ‚‚] [mâ‚ƒ] ...
Stage 1:      [mâ‚€] [mâ‚] [mâ‚‚] [mâ‚ƒ] ...
Stage 2:           [mâ‚€] [mâ‚] [mâ‚‚] [mâ‚ƒ] ...
Stage 3:                [mâ‚€] [mâ‚] [mâ‚‚] [mâ‚ƒ] ...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â†‘ Pipeline bubble (idle time)
```

### Backward Pass

```
1. Gradients flow backward through pipeline
   Stage 3 receives loss â†’ computes grads â†’ sends to Stage 2
   Stage 2 receives grads â†’ computes grads â†’ sends to Stage 1
   Stage 1 receives grads â†’ computes grads â†’ sends to Stage 0

2. Tensor parallel gradients
   Within each stage, for each layer:
   â”œâ”€ Compute local gradients
   â”œâ”€ ALL-REDUCE to synchronize
   â””â”€ Each GPU has full gradient for its parameters

3. Data parallel gradient sync
   After all microbatches processed:
   â””â”€ ALL-REDUCE gradients across data parallel replicas
```

### Weight Update

```
After all microbatches and gradient sync:

1. Each GPU has gradients for its subset of parameters
   (Due to pipeline and tensor parallelism split)

2. Apply optimizer update locally
   GPU_i updates its own parameters
   No communication needed!

3. Next iteration starts with updated weights
```

---

## Configuration Guidelines

### Choosing Dimensions

```
Total GPUs available: N
Model parameters: M
Max layer size: L
Memory per GPU: G

1. Choose Tensor Parallel (T):
   â”œâ”€ If single layer fits: T = 1
   â”œâ”€ If layer needs split: T = 2, 4, or 8
   â””â”€ Constraint: Must have NVLink within T-group
   
   Rule: L / T < G (layer fits after T-way split)

2. Choose Pipeline Parallel (P):
   â”œâ”€ If model fits after T-split: P = 1
   â”œâ”€ If need more splitting: P = 2, 4, 8, 16, ...
   â””â”€ Constraint: P should divide num_layers evenly
   
   Rule: M / (T Ã— P) < G (model fits with T and P)

3. Choose Data Parallel (D):
   â”œâ”€ Use remaining GPUs: D = N / (T Ã— P)
   â””â”€ Higher D = larger effective batch size
   
   Rule: D = N / (T Ã— P)
```

### Example Configurations

#### Small Model (1.3B parameters, 24 layers)

```
GPUs available: 8
Layers: 24
Hidden: 2048

Configuration:
â”œâ”€ T = 1 (layers fit on single GPU)
â”œâ”€ P = 1 (model fits in memory)
â””â”€ D = 8 (use all GPUs for data parallel)

Result: Simple data parallelism
```

#### Medium Model (13B parameters, 40 layers)

```
GPUs available: 64
Layers: 40
Hidden: 5120

Configuration:
â”œâ”€ T = 4 (split wide layers)
â”œâ”€ P = 4 (split into 4 stages of 10 layers)
â””â”€ D = 4 (64 / (4 Ã— 4) = 4)

Result: Balanced 3D parallelism
```

#### Large Model (175B parameters, 96 layers)

```
GPUs available: 1024
Layers: 96
Hidden: 12,288

Configuration:
â”œâ”€ T = 8 (layers very wide, need splitting)
â”œâ”€ P = 16 (96 layers â†’ 6 layers per stage)
â””â”€ D = 8 (1024 / (8 Ã— 16) = 8)

Result: Full 3D parallelism (GPT-3 configuration)
```

#### Extreme Model (1T parameters, 128 layers)

```
GPUs available: 4096
Layers: 128
Hidden: 25,600

Configuration:
â”œâ”€ T = 16 (extremely wide layers)
â”œâ”€ P = 32 (128 layers â†’ 4 layers per stage)
â””â”€ D = 8 (4096 / (16 Ã— 32) = 8)

Result: Maximum parallelization
```

---

## Performance Analysis

### Efficiency Factors

```
Ideal Throughput = GPUs Ã— GPU_FLOPS Ã— Utilization

Real Throughput = Ideal Ã— E_tensor Ã— E_pipeline Ã— E_data

Where:
â”œâ”€ E_tensor: Tensor parallel efficiency
â”‚  â””â”€ Reduced by: ALL-REDUCE overhead
â”‚  â””â”€ Typical: 85-95%
â”‚
â”œâ”€ E_pipeline: Pipeline parallel efficiency
â”‚  â””â”€ Reduced by: Pipeline bubbles
â”‚  â””â”€ Typical: 80-90%
â”‚
â””â”€ E_data: Data parallel efficiency
   â””â”€ Reduced by: Gradient sync
   â””â”€ Typical: 95-98%

Overall: 0.85 Ã— 0.85 Ã— 0.95 â‰ˆ 69% of peak
```

### Pipeline Bubble Analysis

Pipeline efficiency depends on number of microbatches:

```
Pipeline stages: P
Microbatches: M

Ideal time: M Ã— time_per_microbatch
Actual time: (M + P - 1) Ã— time_per_microbatch

Bubble overhead: (P - 1) / (M + P - 1)

Examples:
â”œâ”€ P=4, M=8:  (4-1)/(8+4-1) = 27% bubble
â”œâ”€ P=4, M=16: (4-1)/(16+4-1) = 16% bubble
â”œâ”€ P=4, M=32: (4-1)/(32+4-1) = 9% bubble

Rule: Use M â‰¥ 4Ã—P to keep bubbles < 10%
```

### Memory vs Efficiency Tradeoff

```
More microbatches:
âœ… Better pipeline efficiency
âŒ More activation memory

Fewer microbatches:
âœ… Less memory needed
âŒ Worse pipeline efficiency

Sweet spot: M = 4Ã—P to 8Ã—P
```

---

## Advanced Techniques

### Interleaved Pipeline Scheduling

Instead of assigning consecutive layers to stages, interleave them:

```
Standard: Each stage has consecutive layers
â”œâ”€ Stage 0: Layers 0-23
â”œâ”€ Stage 1: Layers 24-47
â”œâ”€ Stage 2: Layers 48-71
â””â”€ Stage 3: Layers 72-95

Interleaved: Each stage has spread-out layers
â”œâ”€ Stage 0: Layers 0, 4, 8, 12, ..., 92
â”œâ”€ Stage 1: Layers 1, 5, 9, 13, ..., 93
â”œâ”€ Stage 2: Layers 2, 6, 10, 14, ..., 94
â””â”€ Stage 3: Layers 3, 7, 11, 15, ..., 95

Benefits:
â”œâ”€ Reduces pipeline bubble
â”œâ”€ Better load balancing
â””â”€ Can reduce bubble by ~50%
```

### Sequence Parallelism

Split sequence dimension in addition to tensor parallelism:

```
Standard Tensor Parallel:
Input: [batch, sequence, hidden/T]

With Sequence Parallel:
Input: [batch, sequence/S, hidden/T]

Benefits:
â”œâ”€ Supports longer sequences
â”œâ”€ Reduces memory for activations
â””â”€ Better for long-context models

Tradeoff:
â”œâ”€ Additional communication
â””â”€ More complex implementation
```

### Selective Activation Recomputation

Recompute activations instead of storing them:

```
Without recomputation:
â”œâ”€ Store all activations: High memory
â””â”€ Fast backward pass

With full recomputation:
â”œâ”€ Store minimal activations: Low memory
â””â”€ Slow backward pass (recompute everything)

Selective (Megatron-LM approach):
â”œâ”€ Store: Attention scores, layer outputs
â”œâ”€ Recompute: QKV projections, FFN intermediate
â””â”€ Balance: ~30% memory reduction, ~15% slowdown
```

---

## Best Practices

### DO:

âœ… **Use T=8 for large transformers**
   - Optimal for NVLink topology (8 GPUs per node)
   - Good balance of parallelism and efficiency

âœ… **Set P to divide layers evenly**
   - 96 layers â†’ P = 2, 4, 6, 8, 12, 16, 24, 32, 48
   - Uneven splits cause load imbalance

âœ… **Use M â‰¥ 4Ã—P microbatches**
   - Keeps pipeline bubbles < 10%
   - Good memory/efficiency tradeoff

âœ… **Maximize D within memory constraints**
   - Higher effective batch size
   - Better training stability

âœ… **Profile and tune**
   - Measure actual throughput
   - Adjust based on your hardware

### DON'T:

âŒ **Use tensor parallel across nodes**
   - Requires ultra-low latency (NVLink)
   - InfiniBand too slow

âŒ **Make P too large**
   - Pipeline bubbles dominate
   - Need many microbatches

âŒ **Use T=1 for huge layers**
   - Layers won't fit in memory
   - Underutilizes GPUs

âŒ **Ignore pipeline balance**
   - Unequal stage times cause bubbles
   - Slower stages bottleneck whole pipeline

âŒ **Forget about activation memory**
   - Can exceed parameter memory
   - Need selective recomputation

---

## Summary

### The Power of 3D Parallelism

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHY 3D PARALLELISM ENABLES SCALE           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Tensor Parallel (T):                       â”‚
â”‚  â””â”€ Splits wide layers across GPUs          â”‚
â”‚                                             â”‚
â”‚  Pipeline Parallel (P):                     â”‚
â”‚  â””â”€ Splits deep models across GPUs          â”‚
â”‚                                             â”‚
â”‚  Data Parallel (D):                         â”‚
â”‚  â””â”€ Increases batch size and throughput     â”‚
â”‚                                             â”‚
â”‚  Together:                                  â”‚
â”‚  â”œâ”€ Trains trillion-parameter models        â”‚
â”‚  â”œâ”€ Scales to thousands of GPUs             â”‚
â”‚  â”œâ”€ Achieves 70-90% efficiency              â”‚
â”‚  â””â”€ Makes the impossible possible!          â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Takeaways

1. **Orthogonal dimensions** - Each handles different constraint
2. **Independent communication** - Different groups, different patterns
3. **Multiplicative scaling** - Total GPUs = D Ã— P Ã— T
4. **Configuration matters** - Wrong setup kills performance
5. **Hardware awareness** - NVLink for T, InfiniBand for P/D

### The Formula for Success

```
Training Large Models:
1. Choose T based on layer width
2. Choose P based on model depth  
3. Choose D to use remaining GPUs
4. Tune microbatches for efficiency
5. Profile and adjust
```

This is how GPT-3 and beyond became possible! ğŸš€
