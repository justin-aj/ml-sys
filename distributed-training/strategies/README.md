# Distributed Training Strategies

This directory contains implementations and configurations for 6 different distributed training strategies.

---

## ğŸ“ Directory Structure

```
strategies/
â”œâ”€â”€ 1_data_parallel/      - Baseline: Replicate model across GPUs
â”œâ”€â”€ 2_zero_stage1/        - Shard optimizer states (4Ã— memory reduction)
â”œâ”€â”€ 3_zero_stage2/        - Shard optimizer + gradients (8Ã— reduction) â­ RECOMMENDED
â”œâ”€â”€ 4_zero_stage3/        - Shard everything (NÃ— reduction)
â”œâ”€â”€ 5_zero_offload/       - Use CPU memory for optimizer
â””â”€â”€ 6_zero_infinity/      - Use NVMe storage for massive models
```

---

## ğŸ¯ Quick Comparison

| Strategy | Memory Savings | Speed | Best For |
|----------|----------------|-------|----------|
| **1. Data Parallel** | None (baseline) | 1.0Ã— | Small models, testing |
| **2. ZeRO Stage 1** | 29% less | **2.4Ã— faster** | Production (optimizer only) |
| **3. ZeRO Stage 2** | 23% less | **2.4Ã— faster** | **Production (recommended)** â­ |
| **4. ZeRO Stage 3** | Enables 2.2Ã— larger | 1.0Ã— | Large models that don't fit |
| **5. ZeRO-Offload** | 52% less GPU | 0.3Ã— | Limited GPU memory |
| **6. ZeRO-Infinity** | Maximum savings | Slowest | Extremely large models |

---

## ğŸš€ How to Use

Each directory contains:
- **README.md** - Strategy explanation and usage
- **config.json** - DeepSpeed configuration (if applicable)
- **run.sh** - Quick run script

### Running a Strategy

```bash
# Example: Run ZeRO Stage 2 (recommended)
cd 3_zero_stage2
bash run.sh
```

Or use the main script from parent directory:

```bash
# From distributed-training/ directory
# Edit CONFIG in real_model_example.py:
CONFIG = {
    "strategy": "zero2",  # Choose: dp, zero1, zero2, zero3, offload, infinity
    # ...
}

# Run
deepspeed --num_gpus=1 real_model_example.py
```

---

## ğŸ“Š Tested Results (Single GPU - Nov 15, 2025)

### GPT-2 Medium (355M params):

| Strategy | Time/Epoch | GPU Memory | Speedup |
|----------|------------|------------|---------|
| Data Parallel | 125s | 13.62 GB | 1.0Ã— |
| ZeRO-1 | 52s | 9.64 GB | **2.4Ã—** |
| ZeRO-2 | 52s | 10.55 GB | **2.4Ã—** â­ |

### GPT-2 Large (774M params):

| Strategy | Time/Epoch | GPU Memory | Notes |
|----------|------------|------------|-------|
| ZeRO-3 | 105s | 21.03 GB | 2.2Ã— larger model |
| ZeRO-Offload | 375s | 10.10 GB | 52% less memory |

---

## ğŸ“ Learning Path

**Day 1:** Start with `1_data_parallel/` - understand baseline  
**Day 2:** Try `3_zero_stage2/` - see 2.4Ã— speedup!  
**Day 3:** Experiment with `4_zero_stage3/` - train larger models  
**Day 4:** Test `5_zero_offload/` - extreme memory savings

---

## ğŸ“š More Information

- **Main Tutorial**: See `../README.md`
- **Complete Results**: See `../TRAINING_RESULTS.md`
- **Advanced Guide**: See `../REAL_MODELS_GUIDE.md`
- **Cluster Setup**: See `../CLUSTER_QUICKSTART.md`

---

**Questions?** Check the README.md in each strategy directory!
