# ML Systems Projects

A collection of machine learning systems and GPU optimization projects.

---

## ğŸ“‚ Projects

### [mega-kernels/](./mega-kernels/)

**Learn CUDA Mega Kernels with hands-on examples**

A comprehensive tutorial demonstrating kernel fusion optimization techniques for NVIDIA GPUs. Learn how to achieve 1.6x - 1.9x speedups by eliminating memory bandwidth bottlenecks.

#### What are Mega Kernels?

Mega kernels (also called "kernel fusion") combine multiple GPU operations into a single kernel to:
- **Reduce memory bandwidth** by 50% (avoid intermediate global memory roundtrips)
- **Eliminate kernel launch overhead** (1 launch instead of multiple)
- **Keep data in registers** between operations (much faster than global memory)

#### Key Results (Tesla V100-SXM2-32GB)

| Tensor Size | Standard | Mega Kernel | Speedup |
|-------------|----------|-------------|---------|
| 1K elements | 0.0156 ms | 0.0091 ms | **1.72x** |
| 1M elements | 0.0217 ms | 0.0136 ms | **1.59x** |
| 16M elements | 0.3351 ms | 0.1752 ms | **1.91x** |

**Example:** GELU activation + scaling fused into one kernel achieves near-theoretical 2x speedup on large tensors.

#### What's Included

- ğŸ“š **tutorial_mega_kernel.py** - Educational CUDA kernel fusion example
- ğŸ¯ **simple_demo.py** - PyTorch-only demonstration (no custom CUDA)
- ğŸ“– **LEARNING_GUIDE.md** - Step-by-step conceptual tutorial
- ğŸ“Š **BENCHMARK_RESULTS.md** - Detailed V100 performance analysis
- ğŸŒ **REAL_WORLD_USES.md** - Production applications (FlashAttention, Megatron-LM, etc.)

#### Quick Start

```bash
cd mega-kernels
pip install -r requirements.txt
python tutorial_mega_kernel.py
```

**Requirements:** PyTorch >= 2.0.0 with CUDA support

#### Core Concept

```python
# Standard approach (2 kernels, 2 memory roundtrips)
temp = gelu(input)      # GPU kernel 1: read input, write temp to global memory
output = temp * scale   # GPU kernel 2: read temp, write output to global memory

# Mega kernel (1 kernel, 1 memory roundtrip)
output = fused_gelu_scale(input, scale)  # read input, compute both ops in registers, write output
```

**Result:** 50% less memory traffic â†’ up to 1.91x faster on V100

#### Real-World Impact

This technique is used in:
- **FlashAttention** - Attention mechanism optimization (2-4x faster)
- **Megatron-LM** - Training 100B+ parameter models
- **xFormers** - Memory-efficient transformers
- **TensorRT** - Production inference optimization
- **torch.compile()** - PyTorch 2.0 automatic fusion

#### Learn More

---

### [triton-tutorial/](./triton-tutorial/)

**GPU Kernel Fusion with Python (Hands-On) ğŸ”¥**

Learn GPU optimization the modern way - with Triton! Write high-performance GPU kernels in Python and achieve 2-5x speedups over native PyTorch. **Actually works with `pip install`!**

#### What is Triton?

Triton is a Python-based GPU programming framework that:
- **Fuses operations into mega-kernels** (same concept as mega-kernels, but easier!)
- **Keeps data in registers** between operations (avoid slow global memory)
- **Auto-tunes for your GPU** (find optimal block sizes automatically)
- **Production-ready** (used by OpenAI, Meta, HuggingFace, Stability AI)

**The Core Concept:**
```
PyTorch: x â†’ DRAM â†’ exp â†’ DRAM â†’ sum â†’ DRAM â†’ div â†’ DRAM (slow!)
Triton:  x â†’ DRAM â†’ [exp, sum, div in registers] â†’ DRAM (fast!)
```

**Result:** Same math, 2-3x faster, because intermediate values never touch slow memory.

#### Why Triton?

âœ… **Easy installation:** `pip install triton` - actually works!
âœ… **Python-based:** No C++ or CUDA knowledge required
âœ… **90-100% of hand-written CUDA performance**
âœ… **Used in production** by GPT-4, LLAMA, Stable Diffusion
âœ… **Fast iteration:** Instant kernel compilation

#### Expected Results (V100 GPU)

| Operation | PyTorch (ms) | Triton (ms) | Speedup | Impact |
|-----------|-------------|-------------|---------|--------|
| Softmax (4096Ã—4096) | 0.85 | 0.28 | **3.0x** | Fewer kernels + less memory |
| LayerNorm (BERT) | 0.42 | 0.24 | **1.75x** | Used 24x per BERT inference! |
| Flash Attention | 12.5 | 3.8 | **3.3x** | Enables longer sequences |

#### What's Included

- ğŸš€ **simple_fusion.py** - Your first mega-kernel (softmax fusion, 3x speedup)
- ğŸ§  **layer_norm.py** - Transformer layer norm (used in BERT/GPT)
- âš¡ **flash_attention_lite.py** - Advanced fusion (simplified Flash Attention)
- ğŸ“– **LEARNING_GUIDE.md** - Memory hierarchy, fusion patterns, best practices
- ğŸŒ **REAL_WORLD_USES.md** - Production deployments (OpenAI, Meta, HuggingFace)
- ğŸ“‹ **START_HERE.md** - Quick start guide

#### Quick Start (5 minutes)

```bash
# Install (actually this easy!)
pip install triton torch

# Run first tutorial
cd triton-tutorial
python simple_fusion.py

# Expected output:
# PyTorch softmax: 0.847ms
# Triton fused:    0.281ms
# Speedup: 3.01x ğŸš€
```

**Requirements:**
- NVIDIA GPU (compute 7.0+: V100, T4, A100, RTX 20xx/30xx/40xx)
- Python 3.8+
- PyTorch (for comparisons)

#### Tutorial Progression

1. **simple_fusion.py** (10 min) - Softmax fusion basics
2. **layer_norm.py** (20 min) - Real transformer optimization
3. **flash_attention_lite.py** (40 min) - Advanced fusion enabling new algorithms

#### Real-World Impact

**OpenAI:** Powers GPT-4, DALL-E inference (billions of requests/day)
**Meta:** LLAMA models use Triton for fused attention
**HuggingFace:** Flash Attention in transformers library
**Stability AI:** Stable Diffusion optimizations

Example savings for a mid-size ML company:
- Before: 100 GPUs serving transformers = $6,000/day
- After: 75 GPUs (25% faster) = $4,500/day
- **Annual savings: $547,500**

#### vs mega-kernels Tutorial

| Aspect | mega-kernels | triton-tutorial |
|--------|--------------|-----------------|
| **Language** | CUDA C++ | Python |
| **Learning Curve** | Steep | Moderate |
| **Setup** | nvcc compiler | `pip install` |
| **Performance** | 100% baseline | 90-100% |
| **Production Use** | Educational | Production-ready |

**Recommendation:** Learn concepts from `mega-kernels`, build real kernels with `triton-tutorial`.

---

### [tvm-tutorial/](./tvm-tutorial/)

**TVM Concepts & Overview (Reference Only)**

Learn about Apache TVM - an open-source deep learning compiler - through conceptual documentation and examples. **Note:** TVM installation is complex, so this directory provides **educational content without requiring installation**.

#### What is TVM?

TVM (Tensor Virtual Machine) is a compiler framework that:
- **Compiles models for ANY hardware** (NVIDIA, AMD, ARM, TPU, custom accelerators)
- **Auto-tunes schedules** to find optimal performance automatically
- **Enables operator fusion** for memory bandwidth optimization
- **Supports all frameworks** (PyTorch, TensorFlow, ONNX)

**Key Idea:** Write once, deploy everywhere - the ultimate portability.

#### Why Reference Only?

TVM is powerful but has significant installation challenges:
- âŒ Complex build dependencies (LLVM, CMake, CUDA toolkit)
- âŒ Platform-specific compilation issues
- âŒ Hours of debugging for setup
- âŒ Poor pre-built wheel support

**This directory provides conceptual understanding without the installation headache.**

#### What's Included

- ğŸ“– **README.md** - TVM concepts, examples, and comparisons
- ï¿½ **LEARNING_GUIDE.md** - Deep dive into schedules, primitives, auto-tuning
- ğŸŒ **REAL_WORLD_USES.md** - Production deployments (AWS, Meta, AMD)
- ğŸš€ **TVM_ALTERNATIVES.md** - Easier tools with similar capabilities

#### Better Alternatives for Hands-On Learning

Instead of TVM, use these easier tools that teach the same concepts:

| Tool | Installation | Use Case | Difficulty |
|------|--------------|----------|------------|
| **Mega-kernels** | âœ… Working! | GPU optimization, kernel fusion | Moderate |
| **Triton** | `pip install triton` | Modern GPU kernels in Python | Easy |
| **torch.compile()** | Built-in PyTorch 2.0 | Automatic fusion & optimization | Very Easy |
| **JAX/XLA** | `pip install jax` | Functional programming + compiler | Moderate |

**Recommendation:** Master [mega-kernels](./mega-kernels/) first, then explore Triton for production-ready GPU programming.

#### Learning Value

Even without installing TVM, this directory helps you understand:
- âœ… How ML compilers work (separation of computation and optimization)
- âœ… Schedule primitives (tiling, vectorization, parallelization)
- âœ… Auto-tuning strategies (ML-guided search)
- âœ… Cross-platform deployment challenges
- âœ… Production compiler architectures

#### When to Actually Use TVM

Use TVM if you need:
- Cross-compilation to exotic hardware (ARM, custom accelerators)
- Deploy same model to 10+ different platforms
- Academic research requiring TVM specifically
- AWS Neuron SDK or similar TVM-based toolchains

For GPU optimization and kernel fusion, **mega-kernels + Triton** is more practical.

---

## ğŸš€ Getting Started

Clone this repository:
```bash
git clone https://github.com/justin-aj/ml-sys.git
cd ml-sys
```

Navigate to individual project folders for specific setup instructions.

---

## ğŸ“‹ Requirements

- Python 3.8+
- NVIDIA GPU with CUDA support (tested on Tesla V100)
- PyTorch >= 2.0.0

---

## ğŸ“„ License

MIT License - See individual project folders for details.

---

## ğŸ¤ Contributing

Feel free to explore, learn, and adapt these examples for your own projects!
