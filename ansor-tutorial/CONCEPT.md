# Ansor Tutorial: Learned Auto-Scheduling for GPUs

## ğŸ¯ The Core Idea

**Ansor is the ONLY tool that does ALL of this:**

1. âœ… **Loop reordering search** â†’ Automatically explores loop permutations
2. âœ… **Tiling factor exploration** â†’ Searches tile/block sizes, vectorization widths, unrolling factors
3. âœ… **Cross-device cost models** â†’ Learns predictive models of kernel runtime for different GPUs/CPUs
4. âœ… **Learned scheduling** â†’ Trains ML models (gradient-boosted trees) to guide schedule search for unseen workloads

---

## ğŸ¤” Why Not Other Tools?

| Tool | What It Does | What It Lacks |
|------|--------------|---------------|
| **Triton** | JIT compiler + manual kernel authoring, fuses operations | âŒ No schedule search<br>âŒ No learned cost models<br>âŒ Manual optimization |
| **TorchInductor** | Heuristic-based fusion and tiling | âŒ No learned cost model<br>âŒ Fixed heuristics, not adaptive |
| **TensorRT** | Autotunes GEMM/convolution kernels | âŒ Limited to supported layers<br>âŒ Cost model not exposed/generalized |
| **Hidet / MLIR** | Partial search or polyhedral optimization | âŒ Not ML-guided<br>âŒ Limited cross-device learning |

**Bottom Line:** If your goal is **full learned auto-scheduling with cost models and loop exploration across devices**, **TVM + Ansor (or Meta-Scheduler)** is the only open-source system that does this end-to-end.

---

## ğŸ§  What Makes Ansor Unique?

### The Problem Ansor Solves

When optimizing a GPU kernel, you have a **MASSIVE** search space:

```
Loop Order Choices:
  matmul: [i, j, k] vs [k, i, j] vs [i, k, j] vs ...
  â†’ 3! = 6 permutations for just 3 loops
  â†’ Real kernels have 5-10 loops = millions of permutations

Tiling Choices:
  Tile size for i: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  Tile size for j: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  Tile size for k: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  â†’ 11 Ã— 11 Ã— 11 = 1,331 combinations just for tiling!

Vectorization/Unrolling:
  Which loops to vectorize? (4, 8, 16 wide?)
  Which loops to unroll? (2, 4, 8 times?)
  â†’ Hundreds more combinations

Hardware Mapping:
  Which loop maps to threadIdx.x vs blockIdx.x?
  How many threads per block?
  â†’ Device-specific choices

Total Search Space: 10^10 to 10^15 possible schedules! ğŸ¤¯
```

### Ansor's Solution: Machine Learning

Instead of trying random schedules or using fixed heuristics, Ansor:

1. **Samples** a small subset of schedules (e.g., 1000 out of 10^12)
2. **Measures** their actual runtime on your GPU
3. **Learns** a cost model (gradient-boosted tree) that predicts performance
4. **Explores** promising regions of the search space using the learned model
5. **Transfers** knowledge across similar workloads and devices

**Result:** Find near-optimal schedules in minutes instead of days/weeks of manual tuning!

---

## ğŸ”¬ How Ansor Works (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INPUT: Your Computation (e.g., MatMul, Conv2D)          â”‚
â”‚    Written in TVM's compute description language           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SKETCH GENERATION                                        â”‚
â”‚    Ansor generates "sketch templates":                      â”‚
â”‚    - Loop structures (nested loops, tiling patterns)        â”‚
â”‚    - Parallelization strategies (thread/block mapping)      â”‚
â”‚    - Memory hierarchy usage (shared/global/register)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. EVOLUTIONARY SEARCH                                      â”‚
â”‚    For each sketch:                                         â”‚
â”‚    - Sample random schedules (tile sizes, loop orders)      â”‚
â”‚    - Run on GPU and measure time                            â”‚
â”‚    - Train cost model (XGBoost) to predict performance      â”‚
â”‚    - Use model to guide search toward better schedules      â”‚
â”‚    - Iterate 100-10,000 times                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. BEST SCHEDULE FOUND                                      â”‚
â”‚    Output: Optimized CUDA/OpenCL kernel code                â”‚
â”‚    Performance: Often matches or beats hand-tuned kernels!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ What You'll Learn in This Tutorial

### Tutorial 1: **Simple MatMul Auto-Tuning**
- Define a matrix multiplication in TVM
- Run Ansor auto-tuning (1000 trials)
- Compare Ansor schedule vs PyTorch cuBLAS
- Visualize the learned schedule (loop order, tiling)
- **Expected Result:** 80-95% of cuBLAS performance (pretty good for auto-generated!)

### Tutorial 2: **Conv2D Schedule Exploration**
- Define a 2D convolution workload
- Explore different schedule strategies:
  - Spatial tiling vs channel tiling
  - Shared memory usage
  - Vectorization patterns
- See how Ansor chooses different strategies for different input sizes
- **Expected Result:** Understanding why different schedules win for different shapes

### Tutorial 3: **Cross-Device Transfer Learning**
- Tune a kernel on GPU A (e.g., V100)
- Transfer the learned cost model to GPU B (e.g., A100)
- See how Ansor adapts with minimal re-tuning
- **Expected Result:** 50-80% speedup from transfer learning vs tuning from scratch

### Tutorial 4: **Custom Operator Auto-Tuning**
- Define a custom fused operation (e.g., LayerNorm + GELU)
- Let Ansor find the best schedule
- Compare vs hand-written Triton kernel
- **Expected Result:** Competitive performance without manual kernel engineering

---

## ğŸš€ Why This Matters for Real-World ML

### Problem: Every Model Has Unique Kernels

Modern ML models have:
- **Unique shapes:** GPT-4 (vocab=100k), LLaMA (vocab=32k), different batch sizes
- **Custom operators:** FlashAttention, PagedAttention, custom normalizations
- **Multiple devices:** Train on A100, deploy on H100, inference on T4
- **Evolving architectures:** New models appear weekly!

**Manual tuning doesn't scale** â†’ You can't hand-write CUDA kernels for every combination!

### Ansor's Value Proposition

```
Traditional Approach:
  New model â†’ Write CUDA kernel â†’ Tune for weeks â†’ Deploy
  New GPU â†’ Re-tune all kernels â†’ Weeks more work
  New operator â†’ Start from scratch â†’ More weeks
  Total: MONTHS of expert engineering time

Ansor Approach:
  New model â†’ Write TVM compute â†’ Auto-tune overnight â†’ Deploy
  New GPU â†’ Transfer learning â†’ Re-tune hours â†’ Deploy
  New operator â†’ Write TVM compute â†’ Auto-tune â†’ Deploy
  Total: DAYS of work, mostly automated
```

**Real Impact:**
- **OctoML** (founded by TVM creators): Uses Ansor to optimize models for 100+ device types
- **AWS**: Uses TVM/Ansor in SageMaker Neo for edge deployment
- **Meta**: Uses auto-scheduling for PyTorch model optimization

---

## ğŸ“Š Expected Performance

Based on TVM/Ansor papers and real-world usage:

| Workload | Ansor vs Hand-Tuned | Ansor vs PyTorch |
|----------|---------------------|------------------|
| **MatMul (1024Ã—1024)** | 90-95% of cuBLAS | ~1.0x (cuBLAS is gold standard) |
| **Conv2D (ResNet)** | 85-100% of cuDNN | 0.9-1.1x (competitive!) |
| **Custom Fusions** | Often better! | 1.2-2.0x (PyTorch can't fuse) |
| **Sparse/Irregular** | 80-120% of hand-tuned | 1.5-3.0x (no native PyTorch support) |

**Key Insight:** Ansor shines for:
1. Custom operators where hand-tuning is expensive
2. New hardware where libraries aren't optimized yet
3. Fused operations that frameworks can't handle
4. Rapid prototyping (hours vs weeks of kernel engineering)

---

## ğŸ› ï¸ Tutorial Structure

```
ansor-tutorial/
â”œâ”€â”€ README.md                    # Start here
â”œâ”€â”€ CONCEPT.md                   # This file (the "why")
â”œâ”€â”€ INSTALLATION.md              # TVM + Ansor setup (can be tricky!)
â”œâ”€â”€ tutorial_1_matmul.py         # Basic auto-tuning workflow
â”œâ”€â”€ tutorial_2_conv2d.py         # Schedule exploration
â”œâ”€â”€ tutorial_3_transfer.py       # Cross-device learning
â”œâ”€â”€ tutorial_4_custom.py         # Custom operator tuning
â”œâ”€â”€ visualize_schedule.py        # Visualize learned schedules
â”œâ”€â”€ compare_tools.py             # Ansor vs Triton vs PyTorch
â”œâ”€â”€ LEARNING_GUIDE.md            # Deep dive into Ansor concepts
â””â”€â”€ REAL_WORLD_EXAMPLES.md       # Production use cases
```

---

## ğŸ¯ Learning Objectives

After completing this tutorial, you will:

1. âœ… **Understand** the auto-scheduling search space (loops, tiling, parallelization)
2. âœ… **Use** TVM + Ansor to auto-tune kernels on your GPU
3. âœ… **Interpret** learned schedules and understand why they're fast
4. âœ… **Compare** auto-tuning vs manual optimization (Triton)
5. âœ… **Apply** transfer learning across devices
6. âœ… **Know when** to use Ansor vs other tools (Triton, TensorRT, etc.)

---

## ğŸ”® The Future: Meta-Scheduler

**Note:** TVM recently evolved Ansor into **Meta-Scheduler**, which adds:
- Better sketch generation (more diverse search space)
- Improved cost models (neural networks + XGBoost)
- Dynamic shape support (variable batch sizes)
- Better integration with PyTorch/JAX

We'll cover **both** Ansor (simpler, educational) and Meta-Scheduler (cutting-edge) in this tutorial.

---

## ğŸ¤ How This Complements the Triton Tutorial

| Aspect | Triton Tutorial | Ansor Tutorial |
|--------|----------------|----------------|
| **Approach** | Manual kernel engineering | Automated schedule search |
| **Control** | Full control over every detail | High-level compute definition |
| **Speed to solution** | Write kernel â†’ profile â†’ tune (hours-days) | Define compute â†’ auto-tune (minutes-hours) |
| **Performance ceiling** | 95-100% of hand-tuned CUDA | 80-95% of hand-tuned (but automated!) |
| **Best for** | Critical kernels, learning GPU programming | Rapid prototyping, many operators, new hardware |
| **Learning curve** | Moderate (need GPU understanding) | Steeper (need TVM + ML concepts) |

**Use them together!**
- Prototype with Ansor to find good schedules quickly
- Use Triton for final 5-10% optimization of critical kernels
- Or just use Ansor for everything if 90% performance is good enough!

---

## ğŸ¬ Next Steps

Ready to dive in?

1. **Read** `INSTALLATION.md` to set up TVM + Ansor (we'll make it as painless as possible!)
2. **Run** `tutorial_1_matmul.py` to see auto-tuning in action
3. **Explore** the learned schedules and understand what Ansor discovered
4. **Compare** with your Triton kernels to see the trade-offs

Let's unlock the power of **machine learning for kernel optimization**! ğŸš€

---

*"The best GPU kernel is the one you don't have to write yourself."* â€” TVM Philosophy
