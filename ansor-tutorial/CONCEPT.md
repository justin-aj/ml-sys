# Ansor: Learned Auto-Scheduling for GPUs

> **âš ï¸ NOTE:** This is a **concept-only** document for educational reference.  
> **No hands-on tutorial** is provided due to TVM installation complexity.  
> This document explains what Ansor is, why it's unique, and how it compares to other tools like Triton.

---

## ğŸ¯ The Core Idea

**Ansor is the ONLY tool that does ALL of this:**

1. âœ… **Loop reordering search** â†’ Automatically explores loop permutations
2. âœ… **Tiling factor exploration** â†’ Searches tile/block sizes, vectorization widths, unrolling factors
3. âœ… **Cross-device cost models** â†’ Learns predictive models of kernel runtime for different GPUs/CPUs
4. âœ… **Learned scheduling** â†’ Trains ML models (gradient-boosted trees) to guide schedule search for unseen workloads

---

## ğŸ¤” Why Not Other Tools?

| Tool | What It Does | What It Lacks |
|------|--------------|---------------|
| **Triton** | JIT compiler + manual kernel authoring, fuses operations | âŒ No schedule search<br>âŒ No learned cost models<br>âŒ Manual optimization |
| **TorchInductor** | Heuristic-based fusion and tiling | âŒ No learned cost model<br>âŒ Fixed heuristics, not adaptive |
| **TensorRT** | Autotunes GEMM/convolution kernels | âŒ Limited to supported layers<br>âŒ Cost model not exposed/generalized |
| **Hidet / MLIR** | Partial search or polyhedral optimization | âŒ Not ML-guided<br>âŒ Limited cross-device learning |

**Bottom Line:** If your goal is **full learned auto-scheduling with cost models and loop exploration across devices**, **TVM + Ansor (or Meta-Scheduler)** is the only open-source system that does this end-to-end.

---

## ğŸ§  What Makes Ansor Unique?

### The Problem Ansor Solves

When optimizing a GPU kernel, you have a **MASSIVE** search space:

```
Loop Order Choices:
  matmul: [i, j, k] vs [k, i, j] vs [i, k, j] vs ...
  â†’ 3! = 6 permutations for just 3 loops
  â†’ Real kernels have 5-10 loops = millions of permutations

Tiling Choices:
  Tile size for i: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  Tile size for j: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  Tile size for k: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  â†’ 11 Ã— 11 Ã— 11 = 1,331 combinations just for tiling!

Vectorization/Unrolling:
  Which loops to vectorize? (4, 8, 16 wide?)
  Which loops to unroll? (2, 4, 8 times?)
  â†’ Hundreds more combinations

Hardware Mapping:
  Which loop maps to threadIdx.x vs blockIdx.x?
  How many threads per block?
  â†’ Device-specific choices

Total Search Space: 10^10 to 10^15 possible schedules! ğŸ¤¯
```

### Ansor's Solution: Machine Learning

Instead of trying random schedules or using fixed heuristics, Ansor:

1. **Samples** a small subset of schedules (e.g., 1000 out of 10^12)
2. **Measures** their actual runtime on your GPU
3. **Learns** a cost model (gradient-boosted tree) that predicts performance
4. **Explores** promising regions of the search space using the learned model
5. **Transfers** knowledge across similar workloads and devices

**Result:** Find near-optimal schedules in minutes instead of days/weeks of manual tuning!

---

## ğŸ”¬ How Ansor Works (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INPUT: Your Computation (e.g., MatMul, Conv2D)          â”‚
â”‚    Written in TVM's compute description language           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SKETCH GENERATION                                        â”‚
â”‚    Ansor generates "sketch templates":                      â”‚
â”‚    - Loop structures (nested loops, tiling patterns)        â”‚
â”‚    - Parallelization strategies (thread/block mapping)      â”‚
â”‚    - Memory hierarchy usage (shared/global/register)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. EVOLUTIONARY SEARCH                                      â”‚
â”‚    For each sketch:                                         â”‚
â”‚    - Sample random schedules (tile sizes, loop orders)      â”‚
â”‚    - Run on GPU and measure time                            â”‚
â”‚    - Train cost model (XGBoost) to predict performance      â”‚
â”‚    - Use model to guide search toward better schedules      â”‚
â”‚    - Iterate 100-10,000 times                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. BEST SCHEDULE FOUND                                      â”‚
â”‚    Output: Optimized CUDA/OpenCL kernel code                â”‚
â”‚    Performance: Often matches or beats hand-tuned kernels!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ What Ansor Can Do (Conceptual Overview)

### Capability 1: **Simple MatMul Auto-Tuning**
- Define a matrix multiplication in TVM compute language
- Run Ansor auto-tuning (1000+ trials) to explore schedules
- Automatically find loop orders, tile sizes, parallelization strategies
- **Expected Result:** 80-95% of cuBLAS performance (fully auto-generated!)

### Capability 2: **Conv2D Schedule Exploration**
- Define a 2D convolution workload
- Ansor explores different schedule strategies:
  - Spatial tiling vs channel tiling
  - Shared memory usage patterns
  - Vectorization and unrolling factors
- Chooses different strategies for different input shapes automatically
- **Expected Result:** Matches or beats cuDNN for many configurations

### Capability 3: **Cross-Device Transfer Learning**
- Tune a kernel on GPU A (e.g., V100)
- Transfer the learned cost model to GPU B (e.g., A100)
- Ansor adapts with minimal re-tuning using transfer learning
- **Expected Result:** 50-80% faster convergence vs tuning from scratch

### Capability 4: **Custom Operator Auto-Tuning**
- Define custom fused operations (e.g., LayerNorm + GELU)
- Ansor finds optimal schedule without manual kernel engineering
- Competitive with hand-written CUDA/Triton kernels
- **Expected Result:** Competitive performance in hours vs weeks of manual work

---

## ğŸš€ Why This Matters for Real-World ML

### Problem: Every Model Has Unique Kernels

Modern ML models have:
- **Unique shapes:** GPT-4 (vocab=100k), LLaMA (vocab=32k), different batch sizes
- **Custom operators:** FlashAttention, PagedAttention, custom normalizations
- **Multiple devices:** Train on A100, deploy on H100, inference on T4
- **Evolving architectures:** New models appear weekly!

**Manual tuning doesn't scale** â†’ You can't hand-write CUDA kernels for every combination!

### Ansor's Value Proposition

```
Traditional Approach:
  New model â†’ Write CUDA kernel â†’ Tune for weeks â†’ Deploy
  New GPU â†’ Re-tune all kernels â†’ Weeks more work
  New operator â†’ Start from scratch â†’ More weeks
  Total: MONTHS of expert engineering time

Ansor Approach:
  New model â†’ Write TVM compute â†’ Auto-tune overnight â†’ Deploy
  New GPU â†’ Transfer learning â†’ Re-tune hours â†’ Deploy
  New operator â†’ Write TVM compute â†’ Auto-tune â†’ Deploy
  Total: DAYS of work, mostly automated
```

**Real Impact:**
- **OctoML** (founded by TVM creators): Uses Ansor to optimize models for 100+ device types
- **AWS**: Uses TVM/Ansor in SageMaker Neo for edge deployment
- **Meta**: Uses auto-scheduling for PyTorch model optimization

---

## ğŸ“Š Expected Performance

Based on TVM/Ansor papers and real-world usage:

| Workload | Ansor vs Hand-Tuned | Ansor vs PyTorch |
|----------|---------------------|------------------|
| **MatMul (1024Ã—1024)** | 90-95% of cuBLAS | ~1.0x (cuBLAS is gold standard) |
| **Conv2D (ResNet)** | 85-100% of cuDNN | 0.9-1.1x (competitive!) |
| **Custom Fusions** | Often better! | 1.2-2.0x (PyTorch can't fuse) |
| **Sparse/Irregular** | 80-120% of hand-tuned | 1.5-3.0x (no native PyTorch support) |

**Key Insight:** Ansor shines for:
1. Custom operators where hand-tuning is expensive
2. New hardware where libraries aren't optimized yet
3. Fused operations that frameworks can't handle
4. Rapid prototyping (hours vs weeks of kernel engineering)

---

## ğŸ› ï¸ What Would Be Needed (Installation Reality Check)

**Why no hands-on tutorial?** TVM installation is complex:

```bash
# TVM installation challenges:
- Requires building from source (no simple pip install)
- Complex dependencies: LLVM, CUDA toolkit, CMake, C++ compiler
- Platform-specific issues (especially on Windows)
- Build time: 30-60 minutes
- Frequent compatibility issues between TVM/LLVM/CUDA versions
```

**Conceptual file structure** (if TVM were easy to install):
```
ansor-tutorial/
â”œâ”€â”€ CONCEPT.md                   # This file (educational reference)
â”œâ”€â”€ INSTALLATION.md              # Would detail TVM setup (complex!)
â”œâ”€â”€ example_matmul.py            # Would show auto-tuning workflow
â”œâ”€â”€ example_conv2d.py            # Would show schedule exploration
â”œâ”€â”€ example_transfer.py          # Would show cross-device learning
â”œâ”€â”€ example_custom.py            # Would show custom operator tuning
â””â”€â”€ COMPARISON.md                # Ansor vs Triton vs PyTorch
```

**Reality:** Triton tutorial is much more practical for learning GPU optimization!

---

## ğŸ¯ Key Takeaways (Educational)

After understanding this concept, you should know:

1. âœ… **Understand** the auto-scheduling search space is MASSIVE (10^10+ possible schedules)
2. âœ… **Know** Ansor uses machine learning (XGBoost) to navigate this space efficiently
3. âœ… **Recognize** Ansor is the only tool that does learned auto-scheduling end-to-end
4. âœ… **Compare** auto-tuning (Ansor) vs manual optimization (Triton) trade-offs
5. âœ… **Appreciate** transfer learning across devices (V100 â†’ A100)
6. âœ… **Decide when** to use Ansor vs other tools in real-world scenarios

**Practical advice:** For hands-on GPU optimization learning, use the **Triton tutorial** instead!

---

## ğŸ”® The Future: Meta-Scheduler

**Note:** TVM recently evolved Ansor into **Meta-Scheduler**, which adds:
- Better sketch generation (more diverse search space)
- Improved cost models (neural networks + XGBoost)
- Dynamic shape support (variable batch sizes)
- Better integration with PyTorch/JAX

We'll cover **both** Ansor (simpler, educational) and Meta-Scheduler (cutting-edge) in this tutorial.

---

## ğŸ¤ How This Complements the Triton Tutorial

| Aspect | Triton Tutorial | Ansor Tutorial |
|--------|----------------|----------------|
| **Approach** | Manual kernel engineering | Automated schedule search |
| **Control** | Full control over every detail | High-level compute definition |
| **Speed to solution** | Write kernel â†’ profile â†’ tune (hours-days) | Define compute â†’ auto-tune (minutes-hours) |
| **Performance ceiling** | 95-100% of hand-tuned CUDA | 80-95% of hand-tuned (but automated!) |
| **Best for** | Critical kernels, learning GPU programming | Rapid prototyping, many operators, new hardware |
| **Learning curve** | Moderate (need GPU understanding) | Steeper (need TVM + ML concepts) |

**Use them together!**
- Prototype with Ansor to find good schedules quickly
- Use Triton for final 5-10% optimization of critical kernels
- Or just use Ansor for everything if 90% performance is good enough!

---

## ğŸ¬ What to Do Instead

**Want hands-on GPU optimization?**

1. âœ… **Complete the Triton tutorial** (in `../triton-tutorial/`)
   - Practical, pip-installable, runs on your GPU
   - Learn fusion, memory optimization, real kernel engineering
   - See actual 1.3-1.5x speedups on V100

2. ğŸ“š **Read Ansor papers** (conceptual understanding)
   - [Ansor: Generating High-Performance Tensor Programs](https://arxiv.org/abs/2006.06762)
   - [TVM: An Automated End-to-End Optimizing Compiler](https://arxiv.org/abs/1802.04799)

3. ğŸ¢ **Use production tools** (when TVM is already set up)
   - OctoML platform (TVM as-a-service, no installation needed)
   - AWS SageMaker Neo (uses TVM under the hood)
   - Pre-built TVM Docker images (avoid building from source)

**Bottom line:** Understand Ansor conceptually, use Triton practically!

---

*"The best GPU kernel is the one you don't have to write yourself."* â€” TVM Philosophy

---

## ğŸ“š Additional Resources

**Papers:**
- [Ansor: Generating High-Performance Tensor Programs for Deep Learning](https://arxiv.org/abs/2006.06762)
- [Learning to Optimize Tensor Programs](https://arxiv.org/abs/1805.08166)
- [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)

**Production Systems Using Ansor/TVM:**
- **OctoML:** TVM-as-a-service platform for model optimization
- **AWS SageMaker Neo:** Model optimization for edge deployment
- **Meta/Facebook:** PyTorch model optimization backend
- **AMD:** ROCm ML framework optimization

**When You Might Actually Use Ansor:**
- Your company already has TVM infrastructure set up
- Optimizing for exotic/new hardware (RISC-V, custom accelerators)
- Need cross-device optimization (100+ device types)
- Research projects exploring auto-scheduling algorithms
- You have ML engineers who can maintain TVM builds

---

*This document serves as educational reference for understanding the landscape of GPU optimization tools. For practical, hands-on learning, see the Triton tutorial in `../triton-tutorial/`.*
