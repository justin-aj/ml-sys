================================================================================
Mirage Concepts with PyTorch FX: Graph Rewriting & Cost-Based Selection
================================================================================

üìä Original Graph:
graph():
    %x : [num_users=1] = placeholder[target=x]
    %y : [num_users=1] = placeholder[target=y]
    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})
    %mul : [num_users=1] = call_function[target=operator.mul](args = (%add, 2.0), kwargs = {})
    return mul

Original Code:



def forward(self, x, y):
    add = x + y;  x = y = None
    mul = add * 2.0;  add = None
    return mul


================================================================================

üîÑ Starting Equality Saturation...

   Iteration 1: 1 graphs
      ‚ú® Found new variant: original_r1
      ‚ú® Found new variant: original_r3

   Iteration 2: 3 graphs
      ‚ú® Found new variant: original_r3_r1

   Iteration 3: 4 graphs
      ‚ú® Found new variant: original_r3_r1_r1
      ‚ú® Found new variant: original_r3_r1_r2
   E-graph contains 6 equivalent forms

================================================================================
All Generated Variants:
================================================================================

original:



def forward(self, x, y):
    add = x + y;  x = y = None
    mul = add * 2.0;  add = None
    return mul


original_r1:



def forward(self, x, y):
    add = x + y;  x = y = None
    add_1 = add + add;  add = None
    return add_1


original_r3:



def forward(self, x, y):
    add = x + y;  add = None
    mul_1 = x * 2.0;  x = None
    mul_2 = y * 2.0;  y = None
    add_1 = mul_1 + mul_2;  mul_1 = mul_2 = None
    return add_1


original_r3_r1:



def forward(self, x, y):
    add = x + y;  add = None
    add_2 = x + x;  x = None
    mul_2 = y * 2.0;  y = None
    add_1 = add_2 + mul_2;  add_2 = mul_2 = None
    return add_1


original_r3_r1_r1:



def forward(self, x, y):
    add = x + y;  add = None
    add_2 = x + x;  x = None
    add_3 = y + y;  y = None
    add_1 = add_2 + add_3;  add_2 = add_3 = None
    return add_1


original_r3_r1_r2:



def forward(self, x, y):
    add = x + y;  add = None
    mul = x * 2.0;  x = None
    mul_2 = y * 2.0;  y = None
    add_1 = mul + mul_2;  mul = mul_2 = None
    return add_1


================================================================================
‚è±Ô∏è  Benchmarking All Variants:
================================================================================
   original             0.009965 ms/iteration
   original_r1          0.006555 ms/iteration
   original_r3          0.019886 ms/iteration
   original_r3_r1       0.014738 ms/iteration
   original_r3_r1_r1    0.010987 ms/iteration
   original_r3_r1_r2    0.017710 ms/iteration

================================================================================
üìà Results:
================================================================================
‚ú® Best variant:  original_r1 (0.006555 ms)
‚ö†Ô∏è  Worst variant: original_r3 (0.019886 ms)
üöÄ Speedup: 3.03x

üìù Optimal Graph IR:
graph():
    %x : [num_users=1] = placeholder[target=x]
    %y : [num_users=1] = placeholder[target=y]
    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%add, %add), kwargs = {})
    return add_1

üí° Optimal Python Code:



def forward(self, x, y):
    add = x + y;  x = y = None
    add_1 = add + add;  add = None
    return add_1


================================================================================
üîç Verifying Correctness:
================================================================================
   ‚úÖ original             max_diff=0.00e+00
   ‚úÖ original_r1          max_diff=0.00e+00
   ‚úÖ original_r3          max_diff=0.00e+00
   ‚úÖ original_r3_r1       max_diff=0.00e+00
   ‚úÖ original_r3_r1_r1    max_diff=0.00e+00
   ‚úÖ original_r3_r1_r2    max_diff=0.00e+00

‚úÖ All variants produce identical results!

================================================================================
Key Insights:
================================================================================
1. PyTorch FX graphs ‚âà E-graphs (compact representation)
2. Rewrite rules = Algebraic equivalences (x*2 ‚Üî x+x)
3. Cost model = Benchmarking (could be learned neural network)
4. Equality saturation = Apply all rules, pick best

üí° This is exactly how Mirage works, but for complex tensor programs!
================================================================================