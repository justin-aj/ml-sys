# TASO Concept: Graph-Level Optimization

## ğŸ¯ The Big Idea

**TASO optimizes computation graphs using algebraic rewrites.**

Instead of making individual operations faster (like Triton), TASO **eliminates or combines operations** using mathematical identities.

---

## ğŸ“ Concrete Example: The Distributive Property

### **Starting Graph**

```python
Y = (A Â· B) + (A Â· C)
```

**Computation Graph:**
```
    A
   / \
  Â·   Â·
 B     C
  \   /
   +
   |
   Y
```

**Cost:**
- 2 matrix multiplications (expensive!)
- 1 addition
- 2 intermediate tensors stored in memory

---

### **TASO Rewrites It**

Using the distributive property: `AÂ·B + AÂ·C = AÂ·(B+C)`

```python
Y = A Â· (B + C)
```

**Optimized Graph:**
```
    B   C
     \ /
      +
      |
      Â·
      A
      |
      Y
```

**Cost:**
- 1 matrix multiplication (50% reduction!)
- 1 addition
- 1 intermediate tensor

**Savings:**
- **50% fewer FLOPs** (1 matmul vs 2 matmuls)
- **50% less memory** (1 intermediate vs 2)
- **Fewer kernel launches** (2 GPU kernels vs 3)

---

## ğŸ” How TASO Finds This

### **Step 1: Input Graph**
```python
# User writes code
X1 = torch.matmul(A, B)
X2 = torch.matmul(A, C)
Y = X1 + X2
```

### **Step 2: Apply Rewrite Rules**

TASO has a library of algebraic identities:

| Rule | Transformation |
|------|----------------|
| **Distributivity** | `AÂ·B + AÂ·C â†’ AÂ·(B+C)` |
| **Associativity** | `(AÂ·B)Â·C â†’ AÂ·(BÂ·C)` |
| **Commutativity** | `A+B â†’ B+A` |
| **Fusion** | `ReLU(A+B) â†’ ReLU_Add(A,B)` |

### **Step 3: Graph Search**

```
Original Graph
      â”‚
      â–¼
  Apply all rules
      â”‚
      â”œâ”€â”€â–º Candidate 1: AÂ·(B+C)     [Cost: low]
      â”œâ”€â”€â–º Candidate 2: (AÂ·B)+(AÂ·C)  [Cost: high]
      â””â”€â”€â–º Candidate 3: ...
      â”‚
      â–¼
  Select lowest cost
      â”‚
      â–¼
Optimized Graph: AÂ·(B+C)
```

### **Step 4: Cost Estimation**

TASO estimates cost using:
- **FLOPs:** How many operations?
- **Memory:** How many intermediate tensors?
- **Hardware model:** Which GPU? Memory bandwidth?

**Example Cost Model:**
```python
def cost(graph):
    flops = sum(op.flops for op in graph.operations)
    memory = sum(tensor.size for tensor in graph.intermediates)
    kernel_launches = len(graph.operations)
    
    return Î±*flops + Î²*memory + Î³*kernel_launches
```

---

## ğŸ§® Real Numbers: Transformer Attention

### **Original Attention (Simplified)**

```python
# Attention mechanism
Q = Linear1(X)  # matmul
K = Linear2(X)  # matmul
V = Linear3(X)  # matmul
scores = Q @ K.T  # matmul
attn = softmax(scores)
output = attn @ V  # matmul
```

**Cost:** 5 matrix multiplications

### **TASO Optimization**

TASO notices that `Q`, `K`, `V` all multiply the same input `X`:

```python
# Before: 3 separate matmuls
Q = W_Q @ X
K = W_K @ X
V = W_V @ X

# After: 1 batched matmul (TASO rewrite)
QKV = [W_Q; W_K; W_V] @ X  # Concatenated weight matrix
Q, K, V = split(QKV)
```

**Savings:**
- 3 matmuls â†’ 1 matmul (3Ã— reduction!)
- Better GPU utilization (larger batched operation)
- Fewer kernel launches

**Real Speedup:** 1.8-2.2Ã— faster on NVIDIA GPUs (measured)

---

## ğŸ“Š TASO vs Other Optimizers

### **PyTorch JIT (TorchScript)**

```python
# PyTorch does some fusion
@torch.jit.script
def forward(A, B, C):
    return A @ B + A @ C

# Result: Limited fusion (add+matmul maybe)
# Speedup: 1.1-1.3Ã—
```

**Limitation:** Heuristic-based, doesn't explore algebraic rewrites

### **ONNX Runtime**

```python
# ONNX Runtime has fusion patterns
# E.g., Gemm+Add fusion, Conv+BatchNorm fusion

# Speedup: 1.2-1.5Ã—
```

**Limitation:** Fixed fusion patterns, not exhaustive search

### **TASO**

```python
# TASO exhaustively searches algebraic rewrites
optimized = taso.optimize(graph, alpha=1.0, beta=0.5)

# Speedup: 1.5-2.5Ã— (finds non-obvious optimizations!)
```

**Advantage:** Mathematical correctness + exhaustive search = finds optimizations others miss

---

## ğŸ”¬ TASO's Rewrite Rules (Examples)

### **1. Linear Algebra Identities**

```python
# Distributivity
AÂ·B + AÂ·C â†’ AÂ·(B+C)

# Associativity
(AÂ·B)Â·C â†’ AÂ·(BÂ·C)

# Transpose
(AÂ·B)áµ€ â†’ Báµ€Â·Aáµ€
```

### **2. Operator Fusion**

```python
# Add + ReLU
ReLU(A + B) â†’ AddReLU(A, B)

# BatchNorm + ReLU
ReLU(BatchNorm(X)) â†’ BatchNormReLU(X)

# Softmax decomposition
Softmax(X) â†’ Exp(X - Max(X)) / Sum(Exp(X - Max(X)))
```

### **3. Constant Folding**

```python
# Compile-time evaluation
Y = X Â· W  where W is constant
â†’ Precompute parts involving W
```

### **4. Redundancy Elimination**

```python
# Common subexpression elimination
X1 = A Â· B
X2 = A Â· B  # Duplicate!
â†’ X1 = A Â· B; X2 = X1  # Reuse
```

---

## ğŸ¯ When Does TASO Win Big?

### **Best For:**

1. **Transformer Models** (attention has lots of matmul patterns)
   - BERT: 1.5-1.8Ã— speedup
   - GPT-2: 1.6-2.0Ã— speedup
   - Attention blocks: 2-3Ã— speedup

2. **Custom Architectures** (non-standard patterns PyTorch doesn't optimize)
   - Research models with novel operators
   - Domain-specific neural networks

3. **Memory-Constrained Deployment**
   - Edge devices (reduce memory footprint 30-50%)
   - Multi-model serving (fit more models in memory)

### **Less Effective For:**

1. **Simple Sequential Models** (ResNet, VGG)
   - Limited algebraic rewrite opportunities
   - Speedup: 1.1-1.3Ã— (modest)

2. **Single Large Operations** (one giant matmul)
   - No graph-level optimization possible
   - Use kernel-level optimization (Triton) instead

---

## ğŸ”„ The Optimization Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL LEVEL (Architecture)              â”‚  â† Design choices
â”‚  (e.g., use multi-query attention)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRAPH LEVEL (TASO)                      â”‚  â† Algebraic rewrites
â”‚  AÂ·B + AÂ·C â†’ AÂ·(B+C)                     â”‚     1.5-2Ã— speedup
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KERNEL LEVEL (Triton)                   â”‚  â† Fusion + memory opt
â”‚  Fuse softmax operations                 â”‚     1.3-1.5Ã— speedup
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCHEDULE LEVEL (Ansor)                  â”‚  â† Auto-tune loops
â”‚  Find optimal tile sizes                 â”‚     1.2-1.5Ã— speedup
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
           FINAL PERFORMANCE
         (3-5Ã— faster combined!)
```

**Key Insight:** Each level optimizes different aspects. Stack them for maximum performance!

---

## ğŸ’¡ TASO in Production

### **Microsoft**
- Uses TASO for model serving in Azure ML
- Optimizes customer models automatically
- Report: 1.5-2Ã— average speedup on transformers

### **OctoML**
- TASO + TVM for cross-device optimization
- Optimizes same model for 100+ device types
- Reduces deployment time from weeks to hours

### **Facebook/Meta (Research)**
- Explored TASO for PyTorch graph optimization
- Found 1.3-2Ã— speedups on production models
- Some ideas integrated into TorchScript

---

## ğŸ“ Key Takeaways

1. **Graph optimization** is orthogonal to kernel optimization
   - TASO: Reduce number of operations
   - Triton: Make each operation faster
   - **Use both!**

2. **Algebraic rewrites** can find non-obvious optimizations
   - `AÂ·B + AÂ·C â†’ AÂ·(B+C)` seems simple
   - But TASO finds **hundreds** of such patterns in real models

3. **Mathematical correctness** ensures safety
   - All TASO rewrites are mathematically equivalent
   - No approximations (unlike some quantization techniques)

4. **Hardware-aware** optimization matters
   - Same algebraic rewrite may be beneficial on GPU A but not GPU B
   - TASO uses cost models to choose platform-specific optimizations

---

## ğŸ“š Next: See It In Action

Ready to see concrete examples?

1. **`EXAMPLES.md`** - Walk through detailed rewrite examples
2. **`simple_rewrite.py`** - Run the AÂ·B + AÂ·C example
3. **`transformer_attention.py`** - Optimize real attention block

Let's see TASO eliminate operations! ğŸš€

---

*"The fastest operation is the one you never execute."* â€” TASO Philosophy
