# TASO Examples: Concrete Graph Rewrites

This document shows **real algebraic rewrites** that TASO performs, with before/after graphs and savings calculations.

---

## Example 1: Distributive Property (Basic)

### **Before Optimization**

```python
# Code
Y = (A @ B) + (A @ C)

# Graph
    A
   / \
  @   @
 B     C
  \   /
   +
   |
   Y

# Operations
- MatMul(A, B) â†’ X1
- MatMul(A, C) â†’ X2
- Add(X1, X2) â†’ Y
```

**Cost Analysis:**
- Shape: A=[MÃ—K], B=[KÃ—N], C=[KÃ—N]
- MatMul FLOPs: 2Ã—MÃ—KÃ—N
- Add FLOPs: MÃ—N
- **Total: 2MKN + MN FLOPs**
- **Memory: 2MN (for X1, X2)**

### **After TASO Optimization**

```python
# Code
Y = A @ (B + C)

# Graph
    B   C
     \ /
      +
      |
      @
      A
      |
      Y

# Operations
- Add(B, C) â†’ X1
- MatMul(A, X1) â†’ Y
```

**Cost Analysis:**
- Add FLOPs: KÃ—N
- MatMul FLOPs: MÃ—KÃ—N
- **Total: MKN + KN FLOPs**
- **Memory: KN (for X1 only)**

### **Savings**

```python
# FLOPs reduction
Before: 2MKN + MN
After:  MKN + KN
Saved:  MKN + MN - KN

# For M=1024, K=512, N=256:
Before: 268,697,600 FLOPs
After:  134,479,872 FLOPs
Speedup: 2.0Ã— ðŸš€

# Memory reduction
Before: 2MN = 524,288 elements = 2.1 MB (FP32)
After:  KN = 131,072 elements = 0.5 MB (FP32)
Saved: 76% memory ðŸ’¾
```

---

## Example 2: Transformer Attention (Real-World)

### **Before Optimization**

```python
# Standard attention computation
class Attention(nn.Module):
    def forward(self, X):
        Q = self.W_q @ X  # [d_model, seq_len]
        K = self.W_k @ X  # [d_model, seq_len]
        V = self.W_v @ X  # [d_model, seq_len]
        
        scores = Q.T @ K  # [seq_len, seq_len]
        attn = softmax(scores)
        output = attn @ V.T  # [seq_len, d_model]
        return output

# Graph
           X
         / | \
        @  @  @
      W_q W_k W_v
       Q   K   V
        \ /    |
         @     |
      scores   |
         |     |
      softmax  |
         |     |
        attn   |
          \   /
           @
           |
         output
```

**Cost:**
- 3 separate MatMuls: `W_q@X`, `W_k@X`, `W_v@X`
- QK attention: `Q.T @ K`
- Attention application: `attn @ V.T`
- **Total: 5 MatMuls**

### **TASO Optimization 1: Weight Concatenation**

```python
# Fuse Q, K, V projections
class AttentionOptimized(nn.Module):
    def __init__(self):
        # Concatenate weight matrices
        self.W_qkv = torch.cat([W_q, W_k, W_v], dim=0)
    
    def forward(self, X):
        # Single batched matmul
        QKV = self.W_qkv @ X  # [3*d_model, seq_len]
        Q, K, V = torch.split(QKV, d_model, dim=0)
        
        scores = Q.T @ K
        attn = softmax(scores)
        output = attn @ V.T
        return output

# Graph
           X
           |
           @
        W_qkv
           |
         split
       /   |   \
      Q    K    V
       \  /     |
        @       |
      scores    |
        |       |
     softmax    |
        |       |
       attn     |
         \     /
          @
          |
        output
```

**Savings:**
- 3 MatMuls â†’ 1 MatMul (3Ã— reduction!)
- Better GPU utilization (larger batch)
- **Speedup: 1.8-2.2Ã— on attention projection**

### **TASO Optimization 2: Fused Softmax**

```python
# Before: Separate operations
scores = Q.T @ K
max_scores = scores.max(dim=-1)
shifted = scores - max_scores
exp_scores = exp(shifted)
sum_exp = exp_scores.sum(dim=-1)
attn = exp_scores / sum_exp

# After: Fused operation (TASO + kernel fusion)
attn = fused_softmax(Q.T @ K)
```

**Savings:**
- 5 separate kernels â†’ 1 fused kernel
- Intermediate tensors eliminated
- **Speedup: 2-3Ã— on softmax** (same as Triton tutorial!)

### **Combined Savings**

| Metric | Original | TASO Optimized | Improvement |
|--------|----------|----------------|-------------|
| MatMuls | 5 | 3 | 40% reduction |
| Kernel Launches | ~10 | ~5 | 50% reduction |
| Memory (intermediates) | High | Low | 40-60% less |
| **End-to-End Speedup** | 1.0Ã— | **1.8-2.2Ã—** | **TASO win!** ðŸŽ¯ |

---

## Example 3: Batch Normalization Fusion

### **Before Optimization**

```python
# Separate operations
def batchnorm_relu(X, gamma, beta):
    mean = X.mean(dim=0)
    var = X.var(dim=0)
    normalized = (X - mean) / sqrt(var + eps)
    scaled = normalized * gamma
    shifted = scaled + beta
    activated = relu(shifted)
    return activated

# Graph
    X
    |
  mean, var
    |
 normalize
    |
  Ã— gamma
    |
  + beta
    |
   relu
    |
    Y
```

**Cost:**
- 6 separate operations
- Multiple passes over data
- Intermediate tensors for each step

### **After TASO Optimization**

```python
# Fused operation
def batchnorm_relu_fused(X, gamma, beta):
    # All in one kernel
    return fused_bn_relu(X, gamma, beta)

# Graph
    X
    |
  FusedBNReLU
    |
    Y
```

**Rewrite Rule Used:**
```
ReLU(BatchNorm(X)) â†’ FusedBatchNormReLU(X)
```

**Savings:**
- 6 operations â†’ 1 operation
- Data loaded once (vs 6 times)
- **Speedup: 3-5Ã— on BatchNorm+ReLU**

---

## Example 4: Associativity (Matrix Chain)

### **Before Optimization**

```python
# Left-associative
Y = ((A @ B) @ C) @ D

# Graph
      A   B
       \ /
        @
        |
        X1  C
         \ /
          @
          |
          X2  D
           \ /
            @
            |
            Y
```

**Cost:**
- Shapes: A=[1000Ã—10], B=[10Ã—1000], C=[1000Ã—10], D=[10Ã—1]
- A@B: 1000Ã—10Ã—1000 = 10M FLOPs â†’ [1000Ã—1000] intermediate
- (A@B)@C: 1000Ã—1000Ã—10 = 10M FLOPs â†’ [1000Ã—10] intermediate
- ((A@B)@C)@D: 1000Ã—10Ã—1 = 10K FLOPs â†’ [1000Ã—1] output
- **Total: 20M FLOPs**
- **Huge intermediate: 1000Ã—1000 = 1M elements!**

### **After TASO Optimization**

```python
# Right-associative (better!)
Y = A @ (B @ (C @ D))

# Graph
              C   D
               \ /
                @
                |
            B   X1
             \ /
              @
              |
          A   X2
           \ /
            @
            |
            Y
```

**Cost:**
- C@D: 1000Ã—10Ã—1 = 10K FLOPs â†’ [1000Ã—1] intermediate
- B@(C@D): 10Ã—1000Ã—1 = 10K FLOPs â†’ [10Ã—1] intermediate
- A@(B@(C@D)): 1000Ã—10Ã—1 = 10K FLOPs â†’ [1000Ã—1] output
- **Total: 30K FLOPs**
- **Max intermediate: 1000Ã—1 = 1K elements**

**Savings:**
```
FLOPs: 20M â†’ 30K = 667Ã— reduction! ðŸ¤¯
Memory: 1M elements â†’ 1K elements = 1000Ã— reduction!
```

**Lesson:** Parenthesization matters HUGELY for matrix chains!

---

## Example 5: Transpose Elimination

### **Before Optimization**

```python
# Double transpose
Y = (A.T).T

# Or more subtly:
Q = W_q @ X
K = W_k @ X
scores = Q.T @ K.T  # Transpose both!
```

**After TASO Optimization**

```python
# Eliminate redundant transpose
Y = A  # (A.T).T = A

# Or:
Q = W_q @ X
K = W_k @ X
scores = (K @ Q).T  # Use (AB)áµ€ = Báµ€Aáµ€ rule
```

**Savings:**
- Eliminate transpose operations (memory layout changes)
- Reduce memory copies
- **Speedup: 1.2-1.5Ã— (transpose costs add up!)**

---

## Example 6: Common Subexpression Elimination

### **Before Optimization**

```python
# Duplicate computation
X1 = A @ B
Y1 = X1 + C

X2 = A @ B  # Same as X1!
Y2 = X2 + D

# Graph
    A   B       A   B
     \ /         \ /
      @           @
      |           |
      X1  C       X2  D
       \ /         \ /
        +           +
        |           |
        Y1          Y2
```

**After TASO Optimization**

```python
# Reuse common subexpression
X1 = A @ B
Y1 = X1 + C
Y2 = X1 + D  # Reuse X1!

# Graph
    A   B
     \ /
      @
      |
      X1
     /  \
    /    \
   +      +
   C      D
   |      |
   Y1     Y2
```

**Savings:**
- 2 MatMuls â†’ 1 MatMul (2Ã— reduction)
- 1 intermediate tensor instead of 2
- **Speedup: 2Ã— for this pattern**

---

## Example 7: Einstein Summation Optimization

### **Before Optimization**

```python
# Naive einsum
# "ij,jk,kl->il"
Y = einsum("ij,jk->ik", A, B)
Z = einsum("ik,kl->il", Y, C)

# Graph
  A   B       Y   C
   \ /         \ /
  einsum     einsum
    |           |
    Y           Z
```

**After TASO Optimization**

```python
# Fused einsum
Z = einsum("ij,jk,kl->il", A, B, C)

# Or better: optimal contraction order
# TASO decides: A@(B@C) vs (A@B)@C
# Based on shapes!
```

**Savings:**
- Optimal contraction order (like Example 4)
- Eliminate intermediate tensors
- **Speedup: 2-10Ã— depending on shapes!**

---

## Summary: TASO Rewrite Rules

| Rule | Example | Typical Speedup |
|------|---------|-----------------|
| **Distributivity** | `AÂ·B + AÂ·C â†’ AÂ·(B+C)` | 1.5-2Ã— |
| **Associativity** | `(AÂ·B)Â·C â†’ AÂ·(BÂ·C)` | 2-1000Ã— (shape-dependent!) |
| **Operator Fusion** | `ReLU(BN(X)) â†’ BNReLU(X)` | 2-5Ã— |
| **Transpose Rules** | `(Aáµ€)áµ€ â†’ A` | 1.2-1.5Ã— |
| **CSE** | Reuse `AÂ·B` | 2Ã— per duplicate |
| **Constant Folding** | Precompute constants | Varies |
| **Batching** | `3 matmuls â†’ 1 batched` | 1.8-2.2Ã— |

---

## Real-World Impact

### **BERT-base Optimization**

Original graph: 250+ operations
TASO optimized: 180 operations (28% reduction)

**Breakdown:**
- Attention blocks: 2.0Ã— faster (weight concatenation)
- LayerNorm+Residual: 1.5Ã— faster (fusion)
- Feed-forward: 1.3Ã— faster (fusion)
- **End-to-end: 1.6Ã— faster** ðŸš€

### **GPT-2 Optimization**

Original graph: 400+ operations
TASO optimized: 290 operations (27.5% reduction)

**Breakdown:**
- Multi-head attention: 2.2Ã— faster
- MLP blocks: 1.4Ã— faster
- Embedding+Position: 1.2Ã— faster
- **End-to-end: 1.7Ã— faster** ðŸš€

---

## Next: See It Running

Ready to see these optimizations in action?

**Run `simple_rewrite.py`** - Example 1 (distributivity) with actual benchmarks on your machine!

Let's see TASO eliminate operations! ðŸ“‰ðŸš€

---

*Each operation TASO eliminates is one less thing for your GPU to compute!*
