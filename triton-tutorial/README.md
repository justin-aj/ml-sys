# Triton Tutorial: GPU Kernel Fusion & Memory Optimization

## üéØ Core Concept

**The problem with PyTorch:** Every operation launches a separate GPU kernel. Each kernel:
1. Reads data from **global memory** (slow, ~100s of cycles)
2. Does a tiny bit of math
3. Writes results back to **global memory**
4. Repeats for the next operation

**The Triton solution:** Fuse all operations into **one mega-kernel** where:
1. Load data from global memory **once**
2. Keep intermediate values in **registers/shared memory** (fast, ~1 cycle)
3. Do all the math without leaving the chip
4. Write final results back **once**

**Result:** Same answer, 2-5x faster, because you stop wasting time on memory transfers.

---

## üìÅ Tutorial Structure

### Start Here
1. **`simple_fusion.py`** - Your first mega-kernel (softmax fusion)
   - See the memory waste problem
   - Write your first Triton kernel
   - Benchmark native PyTorch vs Triton fusion
   - Expected speedup: 2-3x on modern GPUs

2. **`layer_norm.py`** - Real-world fusion pattern
   - Layer normalization (used in transformers)
   - Shows register reuse patterns
   - Expected speedup: 1.5-2x

3. **`flash_attention_lite.py`** - Advanced fusion (THE KILLER APP!)
   - Simplified Flash Attention concept
   - Online softmax algorithm
   - Block-wise computation without materializing [N√óN] attention matrix
   - Shows how fusion enables fundamentally new algorithms
   - Memory: O(N¬≤) ‚Üí O(N) - enables GPT-4's 32k context!
   - Expected speedup: 2-4x on long sequences

### Deep Dives
4. **`memory_analysis.py`** - Visualize the problem
   - Count memory transactions (PyTorch vs Triton)
   - Bandwidth utilization analysis
   - Shows exactly why fusion wins

### Reference Materials
- **`LEARNING_GUIDE.md`** - Triton concepts explained
- **`REAL_WORLD_USES.md`** - Production deployments
- **`COMMON_PATTERNS.md`** - Reusable kernel templates

---

## üöÄ Installation (Actually Works!)

```bash
# That's it. Seriously.
pip install triton

# Optional: for benchmarking
pip install torch torchvision matplotlib
```

**No CUDA toolkit needed. No CMake. No compilation hell.** Triton is pip-installable and just works.

**Requirements:**
- Python 3.8+
- NVIDIA GPU with compute capability 7.0+ (V100, T4, A100, RTX 20xx/30xx/40xx)
- CUDA drivers (usually already installed if you have PyTorch)

---

## üéì What You'll Learn

### 1. **Memory Hierarchy Reality**
- Why global memory is the bottleneck
- How registers and shared memory work
- Memory bandwidth vs compute capacity

### 2. **Kernel Fusion Patterns**
- Element-wise fusion (add + mul + exp)
- Reduction fusion (softmax, layer norm)
- Online algorithms (Flash Attention)

### 3. **Triton Programming Model**
- Block-level parallelism
- Pointer arithmetic and masks
- Auto-tuning configurations

### 4. **Performance Engineering**
- Benchmarking methodology
- Roofline analysis
- Occupancy optimization

---

## üìä Expected Results

On an NVIDIA V100 (similar to your GPU), you should see:

| Operation | PyTorch (ms) | Triton (ms) | Speedup | Why |
|-----------|-------------|-------------|---------|-----|
| Softmax (4096√ó4096) | 0.85 | 0.28 | **3.0x** | 3 kernels ‚Üí 1 kernel |
| LayerNorm (8192√ó512) | 0.42 | 0.24 | **1.75x** | 5 kernels ‚Üí 1 kernel |
| Flash Attention (seq=2048) | 12.5 | 3.8 | **3.3x** | O(N¬≤) memory ‚Üí O(N) |

The speedup comes from:
- **Fewer kernel launches** (overhead reduction)
- **Less memory traffic** (bandwidth savings)
- **Better cache utilization** (data reuse)

---

## üî• Quick Start

```python
# 1. Run the first tutorial
python simple_fusion.py

# You'll see output like:
# PyTorch softmax: 0.847ms (3 kernel launches, 96 MB memory traffic)
# Triton fused:    0.281ms (1 kernel launch,  32 MB memory traffic)
# Speedup: 3.01x ‚ú®

# 2. Try layer normalization
python layer_norm.py

# 3. See the memory problem visualized
python memory_analysis.py
```

---

## üß† Core Triton Concepts

### Blocks and Programs
```python
@triton.jit
def my_kernel(x_ptr, y_ptr, N, BLOCK_SIZE: tl.constexpr):
    # Each "program" handles one block of data
    pid = tl.program_id(0)  # Which block am I?
    
    # Load a block of data (stays in registers!)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    x = tl.load(x_ptr + offsets, mask=mask)
    
    # Do math (all on-chip, no memory traffic)
    y = x * 2 + 1
    
    # Store results (back to global memory)
    tl.store(y_ptr + offsets, y, mask=mask)
```

**Key insight:** Between `tl.load` and `tl.store`, everything happens in **fast on-chip memory**.

### Memory Hierarchy
```
Registers:       ~1 cycle latency,  KB per SM,   private to thread
Shared Memory:   ~20 cycle latency, 100 KB per SM, shared in thread block
L2 Cache:        ~200 cycle latency, few MB,      shared across GPU
Global Memory:   ~400 cycle latency, GBs,         main GPU RAM (SLOW!)
```

**Triton's magic:** Keep data in registers/shared memory for as long as possible.

---

## üéØ Why Triton vs Alternatives?

| Tool | Installation | Learning Curve | Performance | Use Case |
|------|-------------|----------------|-------------|----------|
| **Triton** | ‚úÖ `pip install` | Medium | 90-100% of hand-written CUDA | Custom kernels for PyTorch |
| **CUDA C++** | ‚ùå Complex setup | Hard | 100% (baseline) | Maximum control, production kernels |
| **TVM** | ‚ùå Build from source | Very Hard | 85-95% | Cross-platform deployment |
| **torch.compile()** | ‚úÖ Built-in | Easy | 70-90% | Quick PyTorch optimization |
| **JAX** | ‚úÖ `pip install` | Medium | 80-90% | Research, NumPy-style code |

**Triton sweet spot:** Production-quality performance with Python-level ease.

---

## üèóÔ∏è Tutorial Philosophy

1. **Show the problem first** - You'll see PyTorch's memory waste
2. **Build the solution** - Write Triton kernels step-by-step
3. **Benchmark everything** - Numbers don't lie
4. **Explain the why** - Understand memory hierarchy, not just copy code

Each tutorial has:
- ‚úÖ **Complete working code** (no placeholders)
- ‚úÖ **Detailed comments** explaining every line
- ‚úÖ **Benchmarking** showing actual speedups
- ‚úÖ **Memory analysis** proving the optimization

---

## üìö Learning Path

### Beginner (1-2 hours)
- Read this README
- Run `simple_fusion.py` and understand the output
- Read `LEARNING_GUIDE.md` sections 1-3

### Intermediate (3-5 hours)
- Complete `layer_norm.py`
- Run `memory_analysis.py` to see the memory problem
- Try modifying block sizes in `simple_fusion.py`

### Advanced (5-10 hours)
- Implement `flash_attention_lite.py`
- Read Flash Attention paper
- Use `auto_tuning.py` to optimize your kernels

### Expert (10+ hours)
- Read `REAL_WORLD_USES.md` for production patterns
- Implement custom kernels for your use case
- Contribute to Triton ecosystem

---

## üîó Resources

- [Triton Documentation](https://triton-lang.org/)
- [Triton GitHub](https://github.com/openai/triton)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)
- [CUDA C++ Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

---

## ü§ù Comparison with mega-kernels Tutorial

**mega-kernels (CUDA C++):**
- Lower-level control
- Harder to write
- Educational for understanding GPU architecture

**triton-tutorial (Triton Python):**
- Higher-level abstractions
- Easier to write and iterate
- Practical for real projects

**Recommendation:** Do `mega-kernels` first to understand the GPU, then use Triton for actual work.

---

## üö¶ Next Steps

1. **Install Triton:** `pip install triton`
2. **Run first tutorial:** `python simple_fusion.py`
3. **See the speedup:** Compare PyTorch vs Triton timings
4. **Read the code:** Understand how fusion eliminates memory traffic
5. **Experiment:** Change block sizes, try different operations

Ready? Let's eliminate some memory bandwidth waste! üöÄ
