"""
Tutorial 1: Simple Fusion - Softmax Mega-Kernel

THE PROBLEM:
PyTorch softmax does 3 separate operations:
1. exp(x) - reads x, writes exp_x to memory
2. sum(exp_x) - reads exp_x, writes sum to memory  
3. exp_x / sum - reads exp_x and sum, writes output

Each step launches a kernel and does a full memory round-trip.
For a 4096x4096 matrix, that's 3 √ó 64MB = 192MB of memory traffic!

THE SOLUTION:
Triton fuses all 3 operations into one kernel:
1. Load x from memory
2. Compute max(x) for numerical stability (stays in registers)
3. Compute exp(x - max) (stays in registers)
4. Compute sum(exp_x) (stays in registers)
5. Compute exp_x / sum (stays in registers)
6. Store final result to memory

Total memory traffic: 64MB read + 64MB write = 128MB (33% less!)
Plus: Only 1 kernel launch instead of 3.

EXPECTED RESULT:
On V100: 2.5-3.5x speedup over PyTorch
"""

import torch
import triton
import triton.language as tl
import time


@triton.jit
def softmax_kernel(
    input_ptr,      # Pointer to input data
    output_ptr,     # Pointer to output data
    input_row_stride,   # How many elements to skip to get to next row
    output_row_stride,
    n_cols,         # Number of columns per row
    BLOCK_SIZE: tl.constexpr,  # Block size (constant for optimization)
):
    """
    Fused softmax kernel that processes one row per program.
    
    Key insight: All intermediate values (max, exp, sum) stay in registers!
    """
    # Each program (thread block) handles one row
    row_idx = tl.program_id(0)
    
    # Calculate starting pointers for this row
    row_start_ptr = input_ptr + row_idx * input_row_stride
    
    # Generate column offsets: [0, 1, 2, ..., BLOCK_SIZE-1]
    col_offsets = tl.arange(0, BLOCK_SIZE)
    
    # Create pointers for all elements in this row
    input_ptrs = row_start_ptr + col_offsets
    
    # Mask for bounds checking (in case n_cols not divisible by BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # === STEP 1: Load data from global memory ===
    # This is the ONLY read from slow global memory
    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
    
    # === STEP 2: Find max (for numerical stability) ===
    # Stays in registers! No memory access.
    row_minus_max = row - tl.max(row, axis=0)
    
    # === STEP 3: Compute exp ===
    # Still in registers!
    numerator = tl.exp(row_minus_max)
    
    # === STEP 4: Compute sum of exp ===
    # Still in registers!
    denominator = tl.sum(numerator, axis=0)
    
    # === STEP 5: Divide to get probabilities ===
    # Still in registers!
    softmax_output = numerator / denominator
    
    # === STEP 6: Store result back to global memory ===
    # This is the ONLY write to slow global memory
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_output, mask=mask)


def triton_softmax(x):
    """
    Wrapper function to launch the Triton kernel.
    """
    n_rows, n_cols = x.shape
    
    # Allocate output tensor
    y = torch.empty_like(x)
    
    # Must be power of 2 and >= n_cols for this simple version
    BLOCK_SIZE = triton.next_power_of_2(n_cols)
    
    # Ensure BLOCK_SIZE is at least 2 (Triton requirement)
    BLOCK_SIZE = max(BLOCK_SIZE, 2)
    
    # Launch one program (thread block) per row
    grid = (n_rows,)
    
    # Launch the kernel
    softmax_kernel[grid](
        x, y,
        x.stride(0), y.stride(0),
        n_cols,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    
    return y


def benchmark_softmax(size=4096, num_runs=100):
    """
    Benchmark PyTorch vs Triton softmax.
    """
    # Create test data
    x = torch.randn(size, size, device='cuda', dtype=torch.float32)
    
    print(f"\n{'='*60}")
    print(f"Benchmarking Softmax Fusion")
    print(f"Matrix size: {size}√ó{size}")
    print(f"Memory per operation: {x.numel() * 4 / 1e6:.1f} MB")
    print(f"{'='*60}\n")
    
    # === PyTorch Softmax ===
    print("üî• PyTorch Native Softmax")
    print("-" * 60)
    
    # Warmup
    for _ in range(10):
        _ = torch.softmax(x, dim=-1)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(num_runs):
        pytorch_output = torch.softmax(x, dim=-1)
    torch.cuda.synchronize()
    pytorch_time = (time.perf_counter() - start) / num_runs * 1000  # ms
    
    # Count operations (this is approximate for educational purposes)
    print(f"Operations performed:")
    print(f"  1. exp(x)       - Launch kernel, read {x.numel()*4/1e6:.1f}MB, write {x.numel()*4/1e6:.1f}MB")
    print(f"  2. sum(exp_x)   - Launch kernel, read {x.numel()*4/1e6:.1f}MB, write {size*4/1e6:.1f}MB")
    print(f"  3. exp_x / sum  - Launch kernel, read {(x.numel() + size)*4/1e6:.1f}MB, write {x.numel()*4/1e6:.1f}MB")
    total_pytorch_memory = (x.numel() * 4) * 3 / 1e6  # Rough estimate
    print(f"  Total memory traffic: ~{total_pytorch_memory:.1f} MB")
    print(f"  Kernel launches: 3")
    print(f"Time: {pytorch_time:.3f} ms")
    
    # === Triton Fused Softmax ===
    print(f"\n{'‚ö°'} Triton Fused Softmax")
    print("-" * 60)
    
    # Warmup
    for _ in range(10):
        _ = triton_softmax(x)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    for _ in range(num_runs):
        triton_output = triton_softmax(x)
    torch.cuda.synchronize()
    triton_time = (time.perf_counter() - start) / num_runs * 1000  # ms
    
    print(f"Operations performed:")
    print(f"  1. Load x")
    print(f"  2. max(x) - stays in registers")
    print(f"  3. exp(x - max) - stays in registers")
    print(f"  4. sum(exp) - stays in registers")
    print(f"  5. exp / sum - stays in registers")
    print(f"  6. Store output")
    total_triton_memory = (x.numel() * 4) * 2 / 1e6  # Read + write only
    print(f"  Total memory traffic: ~{total_triton_memory:.1f} MB")
    print(f"  Kernel launches: 1")
    print(f"Time: {triton_time:.3f} ms")
    
    # === Verify Correctness ===
    max_diff = torch.max(torch.abs(pytorch_output - triton_output)).item()
    print(f"\n{'='*60}")
    print(f"‚úÖ Correctness Check")
    print(f"Max difference: {max_diff:.2e}")
    print(f"Match: {'‚úì PASS' if max_diff < 1e-5 else '‚úó FAIL'}")
    
    # === Performance Summary ===
    speedup = pytorch_time / triton_time
    memory_saved = (total_pytorch_memory - total_triton_memory) / total_pytorch_memory * 100
    
    print(f"\n{'='*60}")
    print(f"üìä Performance Summary")
    print(f"{'='*60}")
    print(f"PyTorch:        {pytorch_time:.3f} ms")
    print(f"Triton:         {triton_time:.3f} ms")
    print(f"Speedup:        {speedup:.2f}x {'üöÄ' if speedup > 1.5 else ''}")
    print(f"Memory saved:   {memory_saved:.1f}%")
    print(f"{'='*60}\n")
    
    # === Explanation ===
    print("üí° Why is Triton faster?")
    print("-" * 60)
    if speedup > 1.5:
        print("‚ú® SUCCESS! Here's why:")
        print(f"  1. Fewer kernel launches: 3 ‚Üí 1 (saves ~{(3-1)*0.005:.3f}ms overhead)")
        print(f"  2. Less memory traffic: {total_pytorch_memory:.0f}MB ‚Üí {total_triton_memory:.0f}MB")
        print(f"  3. Better cache utilization: Data loaded once, reused in registers")
        print(f"  4. Memory bandwidth saved: {memory_saved:.0f}% less DRAM access")
    elif speedup > 1.0:
        print("üëç Modest speedup achieved:")
        print("  - Your GPU might have very fast memory (HBM2)")
        print("  - Or the matrix size isn't large enough to hide overhead")
        print("  - Try larger matrices (8192√ó8192) for better speedups")
    else:
        print("‚ö†Ô∏è  Triton slower than expected:")
        print("  - Possible kernel launch overhead on small matrices")
        print("  - Try larger sizes or check GPU utilization")
    
    return speedup


def memory_access_visualization():
    """
    Visualize the memory access pattern difference.
    """
    print("\n" + "="*60)
    print("üß† Memory Access Pattern Visualization")
    print("="*60)
    
    print("\nüìâ PyTorch (3 separate kernels):")
    print("""
    Step 1: exp(x)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Input  ‚îÇ ‚îÄ‚îÄ> ‚îÇ   GPU   ‚îÇ ‚îÄ‚îÄ> ‚îÇ exp(x)  ‚îÇ
    ‚îÇ (DRAM)  ‚îÇ     ‚îÇ Kernel  ‚îÇ     ‚îÇ (DRAM)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         64 MB                            64 MB
    
    Step 2: sum(exp_x)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ exp(x)  ‚îÇ ‚îÄ‚îÄ> ‚îÇ   GPU   ‚îÇ ‚îÄ‚îÄ> ‚îÇ  sum()  ‚îÇ
    ‚îÇ (DRAM)  ‚îÇ     ‚îÇ Kernel  ‚îÇ     ‚îÇ (DRAM)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         64 MB                            0.016 MB
    
    Step 3: divide
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇexp & sum‚îÇ ‚îÄ‚îÄ> ‚îÇ   GPU   ‚îÇ ‚îÄ‚îÄ> ‚îÇ Output  ‚îÇ
    ‚îÇ (DRAM)  ‚îÇ     ‚îÇ Kernel  ‚îÇ     ‚îÇ (DRAM)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         64 MB                            64 MB
    
    Total DRAM traffic: ~256 MB
    Kernel launches: 3
    """)
    
    print("‚ö° Triton (1 fused kernel):")
    print("""
    Fused softmax kernel
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Input  ‚îÇ ‚îÄ‚îÄ> ‚îÇ   GPU Kernel:        ‚îÇ ‚îÄ‚îÄ> ‚îÇ Output  ‚îÇ
    ‚îÇ (DRAM)  ‚îÇ     ‚îÇ   load x             ‚îÇ     ‚îÇ (DRAM)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   max(x)  ‚óÑ‚îÄ‚îê        ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         64 MB      ‚îÇ   exp()     ‚îÇ In     ‚îÇ          64 MB
                    ‚îÇ   sum()     ‚îÇ Regs!  ‚îÇ
                    ‚îÇ   divide()  ‚îÇ        ‚îÇ
                    ‚îÇ   store    ‚óÑ‚îÄ‚îò        ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    Total DRAM traffic: 128 MB (50% reduction!)
    Kernel launches: 1
    """)
    
    print("\nüí° The Key Insight:")
    print("-" * 60)
    print("PyTorch: x ‚Üí DRAM ‚Üí exp ‚Üí DRAM ‚Üí sum ‚Üí DRAM ‚Üí div ‚Üí DRAM")
    print("Triton:  x ‚Üí DRAM ‚Üí [exp, sum, div all in registers] ‚Üí DRAM")
    print("\nIntermediate values (max, exp, sum) NEVER touch slow memory!")
    print("="*60 + "\n")


if __name__ == "__main__":
    # Show the memory pattern first
    memory_access_visualization()
    
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available. This tutorial requires an NVIDIA GPU.")
        print("If you have a GPU, check your PyTorch installation.")
        exit(1)
    
    # Show GPU info
    print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print()
    
    # Run benchmark
    speedup = benchmark_softmax(size=4096, num_runs=100)
    
    # Try different sizes
    print("\n" + "="*60)
    print("üìà Speedup vs Matrix Size")
    print("="*60)
    for size in [1024, 2048, 4096, 8192]:
        x = torch.randn(size, size, device='cuda')
        
        # Quick benchmark (fewer runs for speed)
        num_runs = 50
        
        # PyTorch
        for _ in range(5):
            _ = torch.softmax(x, dim=-1)
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(num_runs):
            _ = torch.softmax(x, dim=-1)
        torch.cuda.synchronize()
        pytorch_time = (time.perf_counter() - start) / num_runs * 1000
        
        # Triton
        for _ in range(5):
            _ = triton_softmax(x)
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(num_runs):
            _ = triton_softmax(x)
        torch.cuda.synchronize()
        triton_time = (time.perf_counter() - start) / num_runs * 1000
        
        speedup = pytorch_time / triton_time
        print(f"{size:5d}√ó{size:5d}: PyTorch {pytorch_time:6.3f}ms | Triton {triton_time:6.3f}ms | Speedup {speedup:.2f}x")
    
    print("\n‚úÖ Tutorial complete! Key takeaways:")
    print("   1. Fusion eliminates intermediate memory writes")
    print("   2. Registers are 100x+ faster than global memory")
    print("   3. Fewer kernel launches = less overhead")
    print("   4. Same math, less waiting on memory = faster results")
    print("\nüìö Next: Try layer_norm.py for a more complex fusion pattern!")
